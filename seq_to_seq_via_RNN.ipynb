{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c2d4d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "from matching.games import HospitalResident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b9f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_1</th>\n",
       "      <th>f2_1</th>\n",
       "      <th>f3_1</th>\n",
       "      <th>f4_1</th>\n",
       "      <th>f5_1</th>\n",
       "      <th>f6_1</th>\n",
       "      <th>f7_1</th>\n",
       "      <th>f8_1</th>\n",
       "      <th>f9_1</th>\n",
       "      <th>f10_1</th>\n",
       "      <th>f11_1</th>\n",
       "      <th>f12_1</th>\n",
       "      <th>f13_1</th>\n",
       "      <th>f14_1</th>\n",
       "      <th>f15_1</th>\n",
       "      <th>f16_1</th>\n",
       "      <th>f17_1</th>\n",
       "      <th>f18_1</th>\n",
       "      <th>f19_1</th>\n",
       "      <th>f20_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.696794</td>\n",
       "      <td>-0.621645</td>\n",
       "      <td>-2.350322</td>\n",
       "      <td>0.857568</td>\n",
       "      <td>0.183480</td>\n",
       "      <td>0.929362</td>\n",
       "      <td>1.662710</td>\n",
       "      <td>3.002824</td>\n",
       "      <td>-0.952821</td>\n",
       "      <td>0.780778</td>\n",
       "      <td>-2.149981</td>\n",
       "      <td>-0.096125</td>\n",
       "      <td>0.686564</td>\n",
       "      <td>0.932175</td>\n",
       "      <td>1.035598</td>\n",
       "      <td>0.100794</td>\n",
       "      <td>0.472664</td>\n",
       "      <td>0.961984</td>\n",
       "      <td>-1.581007</td>\n",
       "      <td>0.418345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.490176</td>\n",
       "      <td>-1.369651</td>\n",
       "      <td>-6.381362</td>\n",
       "      <td>2.446793</td>\n",
       "      <td>0.648104</td>\n",
       "      <td>5.127069</td>\n",
       "      <td>3.632748</td>\n",
       "      <td>6.645584</td>\n",
       "      <td>-1.694836</td>\n",
       "      <td>1.961207</td>\n",
       "      <td>-5.393690</td>\n",
       "      <td>-1.725572</td>\n",
       "      <td>1.115977</td>\n",
       "      <td>2.599284</td>\n",
       "      <td>1.849126</td>\n",
       "      <td>0.149453</td>\n",
       "      <td>1.651858</td>\n",
       "      <td>2.380088</td>\n",
       "      <td>-2.989520</td>\n",
       "      <td>1.523643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.195791</td>\n",
       "      <td>-1.803062</td>\n",
       "      <td>-8.885239</td>\n",
       "      <td>1.678346</td>\n",
       "      <td>1.484633</td>\n",
       "      <td>7.669556</td>\n",
       "      <td>5.218378</td>\n",
       "      <td>9.897765</td>\n",
       "      <td>-2.370407</td>\n",
       "      <td>2.332558</td>\n",
       "      <td>-7.979431</td>\n",
       "      <td>-2.123378</td>\n",
       "      <td>1.262524</td>\n",
       "      <td>3.922189</td>\n",
       "      <td>2.477547</td>\n",
       "      <td>1.246901</td>\n",
       "      <td>1.795737</td>\n",
       "      <td>2.639566</td>\n",
       "      <td>-4.948484</td>\n",
       "      <td>1.801274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.943051</td>\n",
       "      <td>0.117324</td>\n",
       "      <td>-11.007327</td>\n",
       "      <td>0.045042</td>\n",
       "      <td>2.026570</td>\n",
       "      <td>7.370177</td>\n",
       "      <td>6.339163</td>\n",
       "      <td>11.226333</td>\n",
       "      <td>-2.778230</td>\n",
       "      <td>3.804610</td>\n",
       "      <td>-8.785526</td>\n",
       "      <td>-1.515712</td>\n",
       "      <td>0.418063</td>\n",
       "      <td>2.784484</td>\n",
       "      <td>2.132019</td>\n",
       "      <td>1.920771</td>\n",
       "      <td>1.803917</td>\n",
       "      <td>2.866873</td>\n",
       "      <td>-4.924506</td>\n",
       "      <td>1.716181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.792690</td>\n",
       "      <td>2.802358</td>\n",
       "      <td>-5.212722</td>\n",
       "      <td>-1.270905</td>\n",
       "      <td>0.897933</td>\n",
       "      <td>4.450223</td>\n",
       "      <td>2.796456</td>\n",
       "      <td>7.543190</td>\n",
       "      <td>-1.245164</td>\n",
       "      <td>1.265860</td>\n",
       "      <td>-4.883092</td>\n",
       "      <td>-0.436753</td>\n",
       "      <td>-0.691122</td>\n",
       "      <td>2.223674</td>\n",
       "      <td>-0.473904</td>\n",
       "      <td>0.735597</td>\n",
       "      <td>1.970611</td>\n",
       "      <td>0.547733</td>\n",
       "      <td>-3.001847</td>\n",
       "      <td>0.299120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       f1_1      f2_1       f3_1      f4_1      f5_1      f6_1      f7_1  \\\n",
       "0  1.696794 -0.621645  -2.350322  0.857568  0.183480  0.929362  1.662710   \n",
       "1  4.490176 -1.369651  -6.381362  2.446793  0.648104  5.127069  3.632748   \n",
       "2  6.195791 -1.803062  -8.885239  1.678346  1.484633  7.669556  5.218378   \n",
       "3  5.943051  0.117324 -11.007327  0.045042  2.026570  7.370177  6.339163   \n",
       "4  1.792690  2.802358  -5.212722 -1.270905  0.897933  4.450223  2.796456   \n",
       "\n",
       "        f8_1      f9_1     f10_1     f11_1     f12_1     f13_1     f14_1  \\\n",
       "0   3.002824 -0.952821  0.780778 -2.149981 -0.096125  0.686564  0.932175   \n",
       "1   6.645584 -1.694836  1.961207 -5.393690 -1.725572  1.115977  2.599284   \n",
       "2   9.897765 -2.370407  2.332558 -7.979431 -2.123378  1.262524  3.922189   \n",
       "3  11.226333 -2.778230  3.804610 -8.785526 -1.515712  0.418063  2.784484   \n",
       "4   7.543190 -1.245164  1.265860 -4.883092 -0.436753 -0.691122  2.223674   \n",
       "\n",
       "      f15_1     f16_1     f17_1     f18_1     f19_1     f20_1  \n",
       "0  1.035598  0.100794  0.472664  0.961984 -1.581007  0.418345  \n",
       "1  1.849126  0.149453  1.651858  2.380088 -2.989520  1.523643  \n",
       "2  2.477547  1.246901  1.795737  2.639566 -4.948484  1.801274  \n",
       "3  2.132019  1.920771  1.803917  2.866873 -4.924506  1.716181  \n",
       "4 -0.473904  0.735597  1.970611  0.547733 -3.001847  0.299120  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_EHR1 = pd.read_csv(\"./data_EHR1.csv\")\n",
    "data_EHR1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ccaa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_EHR1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623b940c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_2</th>\n",
       "      <th>f2_2</th>\n",
       "      <th>f3_2</th>\n",
       "      <th>f4_2</th>\n",
       "      <th>f5_2</th>\n",
       "      <th>f15_2</th>\n",
       "      <th>f14_2</th>\n",
       "      <th>f6_2</th>\n",
       "      <th>f17_2</th>\n",
       "      <th>f9_2</th>\n",
       "      <th>f7_2</th>\n",
       "      <th>f12_2</th>\n",
       "      <th>f20_2</th>\n",
       "      <th>f19_2</th>\n",
       "      <th>f11_2</th>\n",
       "      <th>f18_2</th>\n",
       "      <th>f10_2</th>\n",
       "      <th>f16_2</th>\n",
       "      <th>f8_2</th>\n",
       "      <th>f13_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.002464</td>\n",
       "      <td>-0.054770</td>\n",
       "      <td>-1.607723</td>\n",
       "      <td>0.413153</td>\n",
       "      <td>0.526490</td>\n",
       "      <td>0.808553</td>\n",
       "      <td>0.521133</td>\n",
       "      <td>1.489252</td>\n",
       "      <td>1.093870</td>\n",
       "      <td>-0.143866</td>\n",
       "      <td>1.521181</td>\n",
       "      <td>-1.214374</td>\n",
       "      <td>-0.054487</td>\n",
       "      <td>-0.620618</td>\n",
       "      <td>-0.165420</td>\n",
       "      <td>0.877329</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>-0.041809</td>\n",
       "      <td>2.285389</td>\n",
       "      <td>-0.688339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.208255</td>\n",
       "      <td>-0.430312</td>\n",
       "      <td>3.542310</td>\n",
       "      <td>0.273496</td>\n",
       "      <td>0.523185</td>\n",
       "      <td>1.403259</td>\n",
       "      <td>0.807507</td>\n",
       "      <td>-0.765867</td>\n",
       "      <td>0.872294</td>\n",
       "      <td>0.744647</td>\n",
       "      <td>-0.662414</td>\n",
       "      <td>0.211199</td>\n",
       "      <td>-0.622928</td>\n",
       "      <td>0.033095</td>\n",
       "      <td>1.369378</td>\n",
       "      <td>1.721680</td>\n",
       "      <td>-1.527514</td>\n",
       "      <td>-0.234272</td>\n",
       "      <td>0.200160</td>\n",
       "      <td>-0.147486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.177253</td>\n",
       "      <td>-0.359707</td>\n",
       "      <td>7.928680</td>\n",
       "      <td>0.673136</td>\n",
       "      <td>-0.539660</td>\n",
       "      <td>0.083743</td>\n",
       "      <td>0.467750</td>\n",
       "      <td>-3.756303</td>\n",
       "      <td>-0.516528</td>\n",
       "      <td>1.122243</td>\n",
       "      <td>-4.087978</td>\n",
       "      <td>-0.085167</td>\n",
       "      <td>-1.435108</td>\n",
       "      <td>-0.203887</td>\n",
       "      <td>3.237390</td>\n",
       "      <td>1.130163</td>\n",
       "      <td>-3.310383</td>\n",
       "      <td>-1.228359</td>\n",
       "      <td>-3.529958</td>\n",
       "      <td>0.776517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.293687</td>\n",
       "      <td>-0.708697</td>\n",
       "      <td>6.471826</td>\n",
       "      <td>0.595019</td>\n",
       "      <td>-2.137115</td>\n",
       "      <td>-0.089151</td>\n",
       "      <td>-1.102489</td>\n",
       "      <td>-3.336020</td>\n",
       "      <td>-0.365526</td>\n",
       "      <td>0.826688</td>\n",
       "      <td>-4.684949</td>\n",
       "      <td>-0.328677</td>\n",
       "      <td>-1.541318</td>\n",
       "      <td>-0.260947</td>\n",
       "      <td>2.497054</td>\n",
       "      <td>-0.693162</td>\n",
       "      <td>-2.074722</td>\n",
       "      <td>-1.027206</td>\n",
       "      <td>-3.263632</td>\n",
       "      <td>0.990452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941395</td>\n",
       "      <td>-1.305723</td>\n",
       "      <td>3.126918</td>\n",
       "      <td>-0.427142</td>\n",
       "      <td>-2.480852</td>\n",
       "      <td>-0.253852</td>\n",
       "      <td>-1.456013</td>\n",
       "      <td>-1.420847</td>\n",
       "      <td>0.261801</td>\n",
       "      <td>0.014554</td>\n",
       "      <td>-1.940401</td>\n",
       "      <td>-0.648343</td>\n",
       "      <td>-1.986704</td>\n",
       "      <td>-0.346978</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-1.558701</td>\n",
       "      <td>-0.040872</td>\n",
       "      <td>-0.748870</td>\n",
       "      <td>-0.606107</td>\n",
       "      <td>1.572000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       f1_2      f2_2      f3_2      f4_2      f5_2     f15_2     f14_2  \\\n",
       "0  1.002464 -0.054770 -1.607723  0.413153  0.526490  0.808553  0.521133   \n",
       "1 -0.208255 -0.430312  3.542310  0.273496  0.523185  1.403259  0.807507   \n",
       "2 -3.177253 -0.359707  7.928680  0.673136 -0.539660  0.083743  0.467750   \n",
       "3 -3.293687 -0.708697  6.471826  0.595019 -2.137115 -0.089151 -1.102489   \n",
       "4  0.941395 -1.305723  3.126918 -0.427142 -2.480852 -0.253852 -1.456013   \n",
       "\n",
       "       f6_2     f17_2      f9_2      f7_2     f12_2     f20_2     f19_2  \\\n",
       "0  1.489252  1.093870 -0.143866  1.521181 -1.214374 -0.054487 -0.620618   \n",
       "1 -0.765867  0.872294  0.744647 -0.662414  0.211199 -0.622928  0.033095   \n",
       "2 -3.756303 -0.516528  1.122243 -4.087978 -0.085167 -1.435108 -0.203887   \n",
       "3 -3.336020 -0.365526  0.826688 -4.684949 -0.328677 -1.541318 -0.260947   \n",
       "4 -1.420847  0.261801  0.014554 -1.940401 -0.648343 -1.986704 -0.346978   \n",
       "\n",
       "      f11_2     f18_2     f10_2     f16_2      f8_2     f13_2  \n",
       "0 -0.165420  0.877329  0.684882 -0.041809  2.285389 -0.688339  \n",
       "1  1.369378  1.721680 -1.527514 -0.234272  0.200160 -0.147486  \n",
       "2  3.237390  1.130163 -3.310383 -1.228359 -3.529958  0.776517  \n",
       "3  2.497054 -0.693162 -2.074722 -1.027206 -3.263632  0.990452  \n",
       "4 -0.000081 -1.558701 -0.040872 -0.748870 -0.606107  1.572000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_EHR2 = pd.read_csv(\"./data_EHR2.csv\")\n",
    "data_EHR2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c90fc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_EHR2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3dd20",
   "metadata": {},
   "source": [
    "# Get golden-standard-list from permutation matrix for the unmapped features in 2 EHRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7781ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f6_1', 'f7_1', 'f8_1', 'f9_1', 'f10_1', 'f11_1', 'f12_1', 'f13_1', 'f14_1', 'f15_1', 'f16_1', 'f17_1', 'f18_1', 'f19_1', 'f20_1']\n",
      "['f15_2', 'f14_2', 'f6_2', 'f17_2', 'f9_2', 'f7_2', 'f12_2', 'f20_2', 'f19_2', 'f11_2', 'f18_2', 'f10_2', 'f16_2', 'f8_2', 'f13_2']\n"
     ]
    }
   ],
   "source": [
    "# get permutation matrix for the unmapped features in 2 EHRs\n",
    "ump_f_EHR1 = list(data_EHR1.columns[5:])\n",
    "ump_f_EHR2 = list(data_EHR2.columns[5:])\n",
    "\n",
    "print(ump_f_EHR1)\n",
    "print(ump_f_EHR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c5d03f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.zeros((len(ump_f_EHR1), len(ump_f_EHR2)))\n",
    "for i in range(len(ump_f_EHR1)):\n",
    "    for j in range(len(ump_f_EHR2)):\n",
    "        if ump_f_EHR1[i][1:-2] == ump_f_EHR2[j][1:-2]:\n",
    "            p[i][j] = 1\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928975a",
   "metadata": {},
   "source": [
    "# Training seq-to-seq model to regard transformation function as the fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f64b954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer gen_data shape from (60000, 20) to (2500, 24, 20)\n",
    "data_1 = data_EHR1.to_numpy().reshape((2500, 24, 20))\n",
    "data_2 = data_EHR2.to_numpy().reshape((2500, 24, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a874be3",
   "metadata": {},
   "source": [
    "### Define Dataset\n",
    "* input: unmapped feature i in 2 EHRS (totally 15 unmapped features in 2 EHRs, the last 15 columns in data_1/data_2)\n",
    "* target: pre-mapped features in 2 EHRS (the first 5 columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edf1182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq_Dataset(Dataset):\n",
    "    def __init__(self, data, EHR_version): \n",
    "        self.patients_num = data.shape[0]\n",
    "        self.ump_features_list = []\n",
    "        self.mp_features_list = []\n",
    "        \n",
    "        # load mp_features\n",
    "        if os.path.exists(\"./mp_f_tensors_\" + str(EHR_version) + \".pt\"):\n",
    "            self.mp_features_list = torch.load(\"./mp_f_tensors_\" + str(EHR_version) + \".pt\")\n",
    "            print(\"Finish loading mapped features' tensors!\")\n",
    "        else:\n",
    "            for i in range(self.patients_num):\n",
    "                cur_mp_features = torch.tensor(data[i,:,:5])\n",
    "                self.mp_features_list.append(cur_mp_features.float())\n",
    "            \n",
    "            torch.save(self.mp_features_list, \"./mp_f_tensors_\" + str(EHR_version) + \".pt\")\n",
    "            print(\"Finish transforming mapped features' tensors!\")\n",
    "        \n",
    "        # load ump_features\n",
    "        if os.path.exists(\"./ump_f_tensors_\" + str(EHR_version) + \".pt\"):\n",
    "            self.ump_features_list = torch.load(\"./ump_f_tensors_\" + str(EHR_version) + \".pt\")\n",
    "            print(\"Finish loading unmapped features' tensors!\")\n",
    "        else:\n",
    "            for i in range(self.patients_num):\n",
    "                cur_ump_features = torch.tensor(data[i, :, 5:])\n",
    "                self.ump_features_list.append(cur_ump_features.float())\n",
    "                \n",
    "            torch.save(self.ump_features_list, \"./ump_f_tensors_\" + str(EHR_version) + \".pt\")\n",
    "            print(\"Finish transforming unmapped features' tensors!\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.patients_num\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_mp_features = self.mp_features_list[idx]\n",
    "        # shape: [24, 5]\n",
    "        sample_ump_features = self.ump_features_list[idx]\n",
    "        # shape: [24, 15]\n",
    "        return sample_mp_features, sample_ump_features\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a59f08",
   "metadata": {},
   "source": [
    "# Define model_1: feed final hidden state(on the last time stamp of each rnn layer (in our case, the # of rnn layer is only 1), with shape: [1, batch_size, hidden_dim]) to MLP to predict mapped time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daca774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import random\n",
    "# from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53025671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq_model(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(Seq2seq_model, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(1, self.batch_size, hidden_dim)\n",
    "        # RNN to get final hidden states\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, batch_first=True, dropout=0.2)\n",
    "        # MLP to predict pre-mapped features(flatten to shape: seq_len x num_mp_f)\n",
    "        self.target_dim = seq_len * num_mp_f\n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.MLP_drop1 = nn.Dropout(p=0.2)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim / 2))\n",
    "        self.MLP_drop2 = nn.Dropout(p=0.2)\n",
    "        self.dense3 = nn.Linear(int(self.target_dim / 2), self.target_dim)\n",
    "        self.MLP_drop3 = nn.Dropout(p=0.2)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = nn.Flatten()(input_data[0]) \n",
    "        # shape: [10, 24*5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, h_n = self.RNN(unmapped_features, self.h_0)\n",
    "        # h_n: final hidden state with shape [1, batch_size, hidden_dim]\n",
    "        map_predict = self.dense1(h_n[0])\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop1(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop2(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop3(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3bc79b",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dae750",
   "metadata": {},
   "source": [
    "### a. Define train(), val() to call in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "46be890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, batch_size):\n",
    "    size = len(train_loader.dataset)\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        cur_loss = model(data)[\"loss\"]\n",
    "        cur_loss.backward()\n",
    "        optimizer.step()\n",
    "#         if i % 100 == 0:\n",
    "#             loss, current = cur_loss.item(), i*len(data)\n",
    "#             print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "            \n",
    "\n",
    "def val(val_loader, model, batch_size):\n",
    "    val_loss_sum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_loss_sum += model(data)[\"loss\"]\n",
    "    avg_val_loss = val_loss_sum / len(val_loader)\n",
    "    return avg_val_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a917e3da",
   "metadata": {},
   "source": [
    "### b. Split dataset into train+val (90%) & final_test (10%) parts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e675df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish transforming mapped features' tensors!\n",
      "Finish transforming unmapped features' tensors!\n",
      "Finish transforming mapped features' tensors!\n",
      "Finish transforming unmapped features' tensors!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "\n",
    "dataset_1_all = Seq2seq_Dataset(data_1, 1)\n",
    "dataset_2_all = Seq2seq_Dataset(data_2, 2)\n",
    "\n",
    "dataset_1_kfold, dataset_1_final_test = random_split(dataset_1_all, [int(len(dataset_1_all) * 0.9), int(len(dataset_1_all) * 0.1)], generator=torch.Generator().manual_seed(42))\n",
    "dataset_2_kfold, dataset_2_final_test = random_split(dataset_2_all, [int(len(dataset_2_all) * 0.9), int(len(dataset_2_all) * 0.1)], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6eab74",
   "metadata": {},
   "source": [
    "### c. K-Fold cross validation to train and val model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a30112f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "input_dim = 1 # each unmapped feature\n",
    "hidden_dim = 20 # hidden dim of RNN model\n",
    "num_ump_f = 15\n",
    "seq_len = 24\n",
    "num_mp_f = 5\n",
    "lr = 0.002\n",
    "k_folds = 5\n",
    "splits=KFold(n_splits=k_folds, shuffle=True,random_state=42)\n",
    "kfold_test_loss_1 = np.zeros((num_ump_f, k_folds))\n",
    "kfold_test_loss_2 = np.zeros((num_ump_f, k_folds))\n",
    "\n",
    "# trained on unmapped features in EHR1\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_1_kfold)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset_1_kfold, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset_1_kfold, batch_size=batch_size, sampler=test_sampler)\n",
    "    for ump_f_id in range(num_ump_f):\n",
    "        model = Seq2seq_model(ump_f_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        cur_sum_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader, model, optimizer, batch_size)\n",
    "            cur_sum_loss += val(test_loader, model, batch_size)\n",
    "            \n",
    "        kfold_test_loss_1[ump_f_id, fold] = cur_sum_loss / num_epochs\n",
    "        \n",
    "# trained on unmapped features in EHR2\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_2_kfold)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset_2_kfold, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset_2_kfold, batch_size=batch_size, sampler=test_sampler)\n",
    "    for ump_f_id in range(num_ump_f):\n",
    "        model = Seq2seq_model(ump_f_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        cur_sum_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader, model, optimizer, batch_size)\n",
    "            cur_sum_loss += val(test_loader, model, batch_size)\n",
    "            \n",
    "        kfold_test_loss_2[ump_f_id, fold] = cur_sum_loss / num_epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "261f620a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.101517</td>\n",
       "      <td>7.635693</td>\n",
       "      <td>7.959547</td>\n",
       "      <td>9.442474</td>\n",
       "      <td>8.099477</td>\n",
       "      <td>8.116611</td>\n",
       "      <td>9.297311</td>\n",
       "      <td>9.788095</td>\n",
       "      <td>9.765993</td>\n",
       "      <td>9.19864</td>\n",
       "      <td>9.764281</td>\n",
       "      <td>9.684067</td>\n",
       "      <td>8.797012</td>\n",
       "      <td>9.207513</td>\n",
       "      <td>9.466866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      umpf1     umpf2     umpf3     umpf4     umpf5     umpf6     umpf7  \\\n",
       "0  8.101517  7.635693  7.959547  9.442474  8.099477  8.116611  9.297311   \n",
       "\n",
       "      umpf8     umpf9   umpf10    umpf11    umpf12    umpf13    umpf14  \\\n",
       "0  9.788095  9.765993  9.19864  9.764281  9.684067  8.797012  9.207513   \n",
       "\n",
       "     umpf15  \n",
       "0  9.466866  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_test_loss_1_mean = np.mean(kfold_test_loss_1, axis=1)\n",
    "kfold_test_loss_1_mean_df = pd.DataFrame(data=kfold_test_loss_1_mean.reshape(1, 15), columns=[\"umpf\"+str(i) for i in range(1, 16)])\n",
    "kfold_test_loss_1_mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "486d9ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.259842</td>\n",
       "      <td>9.859417</td>\n",
       "      <td>8.137906</td>\n",
       "      <td>9.765788</td>\n",
       "      <td>9.47144</td>\n",
       "      <td>7.583542</td>\n",
       "      <td>9.328816</td>\n",
       "      <td>9.597952</td>\n",
       "      <td>9.267031</td>\n",
       "      <td>8.16348</td>\n",
       "      <td>8.809281</td>\n",
       "      <td>8.174178</td>\n",
       "      <td>9.876471</td>\n",
       "      <td>8.020665</td>\n",
       "      <td>9.889639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      umpf1     umpf2     umpf3     umpf4    umpf5     umpf6     umpf7  \\\n",
       "0  9.259842  9.859417  8.137906  9.765788  9.47144  7.583542  9.328816   \n",
       "\n",
       "      umpf8     umpf9   umpf10    umpf11    umpf12    umpf13    umpf14  \\\n",
       "0  9.597952  9.267031  8.16348  8.809281  8.174178  9.876471  8.020665   \n",
       "\n",
       "     umpf15  \n",
       "0  9.889639  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_test_loss_2_mean = np.mean(kfold_test_loss_2, axis=1)\n",
    "kfold_test_loss_2_mean_df = pd.DataFrame(data=kfold_test_loss_2_mean.reshape(1, 15), columns=[\"umpf\"+str(i) for i in range(1, 16)])\n",
    "kfold_test_loss_2_mean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ca668",
   "metadata": {},
   "source": [
    "### d. Trained the final model using all the 90% training data (data used in the above 5-fold cross-validation process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6d67b33d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shutingcui/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "final_train_dataloader_1 = DataLoader(dataset_1_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = Seq2seq_model(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_1, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./each_umpf_model_param/EHR1/umpf_\"+str(umpf_id+1)+\"_EHR1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "027fd3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "final_train_dataloader_2 = DataLoader(dataset_2_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = Seq2seq_model(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_2, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./each_umpf_model_param/EHR2/umpf_\"+str(umpf_id+1)+\"_EHR2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f38c250",
   "metadata": {},
   "source": [
    "### e. Test the final trained model's performance on 2 EHRs  \n",
    "* using dataset:  dataset_1_final_test & dataset_2_final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5e5ade2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list_ump_EHR1 = []\n",
    "models_list_ump_EHR2 = []\n",
    "for i in range(1, 16):\n",
    "    cur_model_1_pth = \"./each_umpf_model_param/EHR1/umpf_\" + str(i) + \"_EHR1.pth\"\n",
    "    cur_model_2_pth = \"./each_umpf_model_param/EHR2/umpf_\" + str(i) + \"_EHR2.pth\"\n",
    "    \n",
    "    cur_model_1 = Seq2seq_model(i-1, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_1.load_state_dict(torch.load(cur_model_1_pth))\n",
    "    \n",
    "    cur_model_2 = Seq2seq_model(i-1, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_2.load_state_dict(torch.load(cur_model_2_pth))\n",
    "    \n",
    "    models_list_ump_EHR1.append(cur_model_1)\n",
    "    models_list_ump_EHR2.append(cur_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d18f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_1_test = DataLoader(dataset_1_final_test, batch_size=10)\n",
    "dataloader_2_test = DataLoader(dataset_2_final_test, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3fd3fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dl_1 = len(dataloader_1_test)\n",
    "len_dl_2 = len(dataloader_2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "186c01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_test_1 = len(dataloader_1_test.dataset)\n",
    "patients_test_2 = len(dataloader_2_test.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc60518",
   "metadata": {},
   "source": [
    "# Compute [num_of_ump, num_of_mp*sel_len] matrix for both EHR1 and EHR2, with each row calculated as the average predicted matched features' values for current unmapped feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "99db9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ump_f_1 = 15\n",
    "num_ump_f_2 = 15\n",
    "num_mp_f = 5\n",
    "seq_len = 24\n",
    "ump_mp_matrix_1 = np.zeros((num_ump_f_1, seq_len * num_mp_f))\n",
    "ump_mp_matrix_2 = np.zeros((num_ump_f_2, seq_len * num_mp_f))\n",
    "for ump_id_1 in range(num_ump_f_1):\n",
    "    cur_model_1 = models_list_ump_EHR1[ump_id_1]\n",
    "    cur_model_1.eval()\n",
    "    for data in dataloader_1_test:\n",
    "        cur_pred_map = cur_model_1(data)[\"predicts\"] # (10, 120)\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy(), axis=0) # [120]\n",
    "        ump_mp_matrix_1[ump_id_1][:] += cur_pred_map_batch_sum\n",
    "\n",
    "for ump_id_2 in range(num_ump_f_2):\n",
    "    cur_model_2 = models_list_ump_EHR2[ump_id_2]\n",
    "    cur_model_2.eval()\n",
    "    for data in dataloader_2_test:\n",
    "        cur_pred_map = cur_model_2(data)[\"predicts\"]\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy(), axis=0)\n",
    "        ump_mp_matrix_2[ump_id_2][:] += cur_pred_map_batch_sum\n",
    "ump_mp_matrix_1 = ump_mp_matrix_1 / patients_test_1\n",
    "ump_mp_matrix_2 = ump_mp_matrix_2 / patients_test_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc02f20",
   "metadata": {},
   "source": [
    "# Get cosine similarity matrix via pairwise.cosine_similarity, and apply Gale-Shapley algorithm based on cosine similarity matrix to get predicted matched pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4c612817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "cos_sim_matrix_1_to_2 = pairwise.cosine_similarity(ump_mp_matrix_1, ump_mp_matrix_2)\n",
    "cos_sim_matrix_2_to_1 = pairwise.cosine_similarity(ump_mp_matrix_2, ump_mp_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "15f58a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Matching_via_HRM(C_X1_train, C_X2_train, P_x1_O_to_R, num_mapped_axis):  # in this case here the small feature sized database is X1, so we need to treat it as hospital and there will be capacities on it.\n",
    "    ####### ----------  X1 train ------------- ##########\n",
    "\n",
    "    true_features_pref_X1_train = {}\n",
    "    cross_recon_features_pref_X1_train = {}\n",
    "    capacities_X1_train = {}\n",
    "\n",
    "    for i in range(C_X1_train.shape[0]):  # C_X1_train.shape[0]: number of unmapped features in dataset_1\n",
    "        sorted_index = np.argsort(-C_X1_train[i, :])\n",
    "        sorted_col_index = [\"C\" + str(sorted_index[v] + 1) for v in range(len(sorted_index))]\n",
    "        true_features_pref_X1_train[\"R\" + str(i + 1)] = sorted_col_index\n",
    "        capacities_X1_train[\"R\" + str(i + 1)] = 1\n",
    "\n",
    "    for j in range(C_X1_train.shape[1]): # C_X1_train.shape[1]:  number of unmapped features in dataset_2\n",
    "        sorted_index = np.argsort(-C_X1_train[:, j])\n",
    "        sorted_col_index = [\"R\" + str(sorted_index[v] + 1) for v in range(len(sorted_index))]\n",
    "        cross_recon_features_pref_X1_train[\"C\" + str(j + 1)] = sorted_col_index\n",
    "\n",
    "    game_X1_train = HospitalResident.create_from_dictionaries(cross_recon_features_pref_X1_train,\n",
    "                                                              true_features_pref_X1_train,\n",
    "                                                              capacities_X1_train)\n",
    "\n",
    "    ####### ----------  X2 train ------------- ##########\n",
    "\n",
    "    true_features_pref_X2_train = {}\n",
    "    cross_recon_features_pref_X2_train = {}\n",
    "    capacities_X2_train = {}\n",
    "\n",
    "    for i in range(C_X2_train.shape[0]):  # C_X2_train.shape[0]: number of unmapped features in dataset_2\n",
    "        sorted_index = np.argsort(-C_X2_train[i, :])\n",
    "        sorted_col_index = [\"C\" + str(sorted_index[v] + 1) for v in range(len(sorted_index))]\n",
    "        true_features_pref_X2_train[\"R\" + str(i + 1)] = sorted_col_index\n",
    "\n",
    "    for j in range(C_X2_train.shape[1]):  # C_X2_train.shape[1]: number of unmapped features in dataset_1\n",
    "        sorted_index = np.argsort(-C_X2_train[:, j])\n",
    "        sorted_col_index = [\"R\" + str(sorted_index[v] + 1) for v in range(len(sorted_index))]\n",
    "        cross_recon_features_pref_X2_train[\"C\" + str(j + 1)] = sorted_col_index\n",
    "        capacities_X2_train[\"C\" + str(j + 1)] = 1\n",
    "\n",
    "    # create_from_dictionaries(resident_prefs, hospital_prefs, capacities, clean=False)\n",
    "    game_X2_train = HospitalResident.create_from_dictionaries(true_features_pref_X2_train,\n",
    "                                                              cross_recon_features_pref_X2_train,\n",
    "                                                              capacities_X2_train)\n",
    "\n",
    "       ######   ------------  Final matching -----------   ##########\n",
    "\n",
    "    print(\"\\n ------- Matching from X1_train  --------- \\n\")\n",
    "    matching_x1_train = game_X1_train.solve()\n",
    "    print(matching_x1_train)\n",
    "\n",
    "    print(\"\\n ------- Matching from X2_train  --------- \\n\")\n",
    "    matching_x2_train = game_X2_train.solve()\n",
    "    print(matching_x2_train)\n",
    "    x1_train_y = [int(str(v[0])[1:]) if v else None for v in matching_x1_train.values()]\n",
    "    x2_train_y = [int(str(v[0])[1:]) if v else None for v in matching_x2_train.values()]\n",
    "\n",
    "    # matching matrices\n",
    "    matching_x1_train_matrix = np.zeros(C_X1_train.shape)\n",
    "    # shape: [num_unmapped_features_in_d1, num_unmapped_features_in_d2]\n",
    "    matching_x2_train_matrix = np.zeros(np.transpose(C_X2_train).shape)\n",
    "    # shape: [num_unmapped_features_in_d1, num_unmapped_features_in_d2]\n",
    "\n",
    "    for i in range(matching_x1_train_matrix.shape[0]):  # number of unmapped features in d_1\n",
    "        if x1_train_y[i] is not None:\n",
    "            matching_x1_train_matrix[i, x1_train_y[i] - 1] = 1  # shape: [# of ump features in d1, # of ump features in d2]\n",
    "        # unmapped feature i in d_1 and unmapped feature \"x1_train_y[i] - 1\" in d_2 has a match\n",
    "\n",
    "    for i in range(matching_x2_train_matrix.shape[0]):  # number of unmapped features in d_1\n",
    "        if x2_train_y[i] is not None:\n",
    "            matching_x2_train_matrix[i, x2_train_y[i] - 1] = 1  # shape: [# of ump features in d1, # of ump features in d2]\n",
    "        # unmapped feature i in d_1 and unmapped feature \"x2_train_y[i] - 1\" in d_2 has a match\n",
    "    # getting the number of correct matches that had a match in other database\n",
    "    num_correct_from_x1 = 0\n",
    "    num_correct_from_x2 = 0\n",
    "    for i in range(P_x1_O_to_R.shape[0]):  # number of unmapped features in d_1\n",
    "        if np.all(P_x1_O_to_R[i] == matching_x1_train_matrix[i]):\n",
    "            # only when the positions of 0-1 are exactly the same, will this condition be true\n",
    "            num_correct_from_x1 = num_correct_from_x1 + 1\n",
    "        if np.all(P_x1_O_to_R[i] == matching_x2_train_matrix[i]):\n",
    "            num_correct_from_x2 = num_correct_from_x2 + 1\n",
    "\n",
    "    return num_correct_from_x1, num_correct_from_x2, matching_x1_train_matrix, matching_x2_train_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4570d12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C10], R2: [C5], R3: [C15], R4: [C6], R5: [C8], R6: [C1], R7: [C13], R8: [C3], R9: [C12], R10: [C4], R11: [C2], R12: [C7], R13: [C14], R14: [C11], R15: [C9]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R10], C2: [R5], C3: [R15], C4: [R6], C5: [R8], C6: [R1], C7: [R13], C8: [R3], C9: [R12], C10: [R4], C11: [R2], C12: [R7], C13: [R14], C14: [R11], C15: [R9]}\n"
     ]
    }
   ],
   "source": [
    "correct_with_match_from_x1_test, correct_with_match_from_x2_test, x1_match_matrix_test, x2_match_matrix_test = Matching_via_HRM(cos_sim_matrix_1_to_2, cos_sim_matrix_2_to_1, p, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bea86c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim cor F values  0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "TP_x1 = 0\n",
    "FP_x1 = 0\n",
    "TN_x1 = 0\n",
    "FN_x1 = 0\n",
    "for i in range(p.shape[0]):\n",
    "    for j in range(p.shape[1]):\n",
    "        if (p[i, j] == 1) & (x1_match_matrix_test[i, j] == 1):\n",
    "            TP_x1 = TP_x1 + 1\n",
    "        elif (p[i, j] == 1) & (x1_match_matrix_test[i, j] == 0):\n",
    "            FN_x1 = FN_x1 + 1\n",
    "        elif (p[i, j] == 0) & (x1_match_matrix_test[i, j] == 0):\n",
    "            TN_x1 = TN_x1 + 1\n",
    "        elif (p[i, j] == 0) & (x1_match_matrix_test[i, j] == 1):\n",
    "            FP_x1 = FP_x1 + 1\n",
    "\n",
    "TP_x2 = 0\n",
    "FP_x2 = 0\n",
    "TN_x2 = 0\n",
    "FN_x2 = 0\n",
    "for i in range(p.shape[0]):\n",
    "    for j in range(p.shape[1]):\n",
    "        if (p[i, j] == 1) & (x2_match_matrix_test[i, j] == 1):\n",
    "            TP_x2 = TP_x2 + 1\n",
    "        elif (p[i, j] == 1) & (x2_match_matrix_test[i, j] == 0):\n",
    "            FN_x2 = FN_x2 + 1\n",
    "        elif (p[i, j] == 0) & (x2_match_matrix_test[i, j] == 0):\n",
    "            TN_x2 = TN_x2 + 1\n",
    "        elif (p[i, j] == 0) & (x2_match_matrix_test[i, j] == 1):\n",
    "            FP_x2 = FP_x2 + 1\n",
    "F1_fromx1 = (2 * TP_x1) / (2 * TP_x1 + FN_x1 + FP_x1)\n",
    "F1_fromx2 = (2 * TP_x2) / (2 * TP_x2 + FN_x2 + FP_x2)\n",
    "\n",
    "print(\"Sim cor F values \", F1_fromx1, F1_fromx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "43d77518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Former method to get avg_cos_sim_matrix\n",
    "\n",
    "# num_ump_f_1 = 15\n",
    "# num_ump_f_2 = 15\n",
    "# avg_cos_sim_matrix = np.zeros((num_ump_f_1, num_ump_f_2))\n",
    "# for ump_i_1 in range(15):\n",
    "#     cur_model_1 = models_list_ump_EHR1[ump_i_1]\n",
    "#     cur_model_1.eval()\n",
    "#     cur_umpf_1_to_all_umpf_2 = np.zeros((len_dl_1, num_ump_f_2))\n",
    "#     for index, (data1, data2) in enumerate(zip(dataloader_1_test, dataloader_2_test)):\n",
    "#         pred_map_1 = cur_model_1(data1)[\"predicts\"] \n",
    "#         pred_map_1_asarray = pred_map_1.detach().numpy() # (10, 120)\n",
    "#         for ump_i_2 in range(15):\n",
    "#             cur_model_2 = models_list_ump_EHR2[ump_i_2]\n",
    "#             cur_model_2.eval()\n",
    "#             pred_map_2 = cur_model_2(data2)[\"predicts\"]\n",
    "#             pred_map_2_asarray = pred_map_2.detach().numpy() # (10, 120)\n",
    "#             # compute cosine similarity on cur batch\n",
    "#             cur_cosine_sim = np.sum(pred_map_1_asarray*pred_map_2_asarray, axis=1) / (norm(pred_map_1_asarray, axis=1)*norm(pred_map_2_asarray, axis=1))\n",
    "#             cur_umpf_1_to_all_umpf_2[index][ump_i_2] = np.average(cur_cosine_sim)\n",
    "#     avg_cos_sim_matrix[ump_i_1, :] = np.average(cur_umpf_1_to_all_umpf_2, axis=0)\n",
    "\n",
    "    \n",
    "# print(avg_cos_sim_matrix)\n",
    " \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d444d81",
   "metadata": {},
   "source": [
    "# The predicted matched features for each unmapped feature in EHR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4b59b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_matched_for_umf1 = avg_cos_sim_matrix.argmax(axis = 1)\n",
    "# pred_matched_for_umf1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dea4db",
   "metadata": {},
   "source": [
    "# The actual matched feature for each unmapped feature in EHR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e6a6500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_matched_for_umf1 = p.argmax(axis=1)\n",
    "# actual_matched_for_umf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf8a17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_perf = (pred_matched_for_umf1 == actual_matched_for_umf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ae8eb91d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predicted_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9caa444",
   "metadata": {},
   "source": [
    "# Define model_2: feed the output(the final hidden state of the last layer on each time stamp, with shape: [batch_size, seq_len(24), hidden_dim]) to MLP to predict mapped time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5aefd9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq_model_pass_final_hs_each_time(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(Seq2seq_model_pass_final_hs_each_time, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(1, self.batch_size, hidden_dim)\n",
    "        # RNN to get final hidden states\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        # MLP to predict pre-mapped features(flatten to shape: seq_len x num_mp_f)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.MLP_drop1 = nn.Dropout(p=0.2)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.MLP_drop2 = nn.Dropout(p=0.2)\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.MLP_drop3 = nn.Dropout(p=0.2)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop1(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop2(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop3(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3904197",
   "metadata": {},
   "source": [
    "# K-fold cross validation on the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "165798fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "input_dim = 1 # each unmapped feature\n",
    "hidden_dim = 20 # hidden dim of RNN model\n",
    "num_ump_f = 15\n",
    "seq_len = 24\n",
    "num_mp_f = 5\n",
    "lr = 0.002\n",
    "k_folds = 5\n",
    "splits=KFold(n_splits=k_folds, shuffle=True,random_state=42)\n",
    "kfold_test_model2_loss_1 = np.zeros((num_ump_f, k_folds))\n",
    "kfold_test_model2_loss_2 = np.zeros((num_ump_f, k_folds))\n",
    "\n",
    "# trained on unmapped features in EHR1\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_1_kfold)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset_1_kfold, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset_1_kfold, batch_size=batch_size, sampler=test_sampler)\n",
    "    for ump_f_id in range(num_ump_f):\n",
    "        model = Seq2seq_model_pass_final_hs_each_time(ump_f_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        cur_sum_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader, model, optimizer, batch_size)\n",
    "            cur_sum_loss += val(test_loader, model, batch_size)\n",
    "            \n",
    "        kfold_test_model2_loss_1[ump_f_id, fold] = cur_sum_loss / num_epochs\n",
    "        \n",
    "# trained on unmapped features in EHR2\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_2_kfold)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset_2_kfold, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset_2_kfold, batch_size=batch_size, sampler=test_sampler)\n",
    "    for ump_f_id in range(num_ump_f):\n",
    "        model = Seq2seq_model_pass_final_hs_each_time(ump_f_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        cur_sum_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader, model, optimizer, batch_size)\n",
    "            cur_sum_loss += val(test_loader, model, batch_size)\n",
    "            \n",
    "        kfold_test_model2_loss_2[ump_f_id, fold] = cur_sum_loss / num_epochs\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ba011",
   "metadata": {},
   "source": [
    "# average validation loss on unmapped features in EHR1 using model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "beecab9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.677182</td>\n",
       "      <td>3.803522</td>\n",
       "      <td>4.200935</td>\n",
       "      <td>8.242604</td>\n",
       "      <td>4.604284</td>\n",
       "      <td>5.171813</td>\n",
       "      <td>7.595304</td>\n",
       "      <td>9.782699</td>\n",
       "      <td>9.550829</td>\n",
       "      <td>7.344407</td>\n",
       "      <td>9.569802</td>\n",
       "      <td>9.113071</td>\n",
       "      <td>6.269675</td>\n",
       "      <td>7.402231</td>\n",
       "      <td>8.328141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      umpf1     umpf2     umpf3     umpf4     umpf5     umpf6     umpf7  \\\n",
       "0  4.677182  3.803522  4.200935  8.242604  4.604284  5.171813  7.595304   \n",
       "\n",
       "      umpf8     umpf9    umpf10    umpf11    umpf12    umpf13    umpf14  \\\n",
       "0  9.782699  9.550829  7.344407  9.569802  9.113071  6.269675  7.402231   \n",
       "\n",
       "     umpf15  \n",
       "0  8.328141  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_val_loss_data1 = np.mean(kfold_test_model2_loss_1, axis=1)\n",
    "avg_val_loss_data2 = np.mean(kfold_test_model2_loss_2, axis=1)\n",
    "avg_val_loss_umpf_1 = pd.DataFrame(data=avg_val_loss_data1.reshape(1, 15), columns=[\"umpf\"+str(i) for i in range(1, 16)])\n",
    "avg_val_loss_umpf_2 = pd.DataFrame(data=avg_val_loss_data2.reshape(1, 15), columns=[\"umpf\"+str(i) for i in range(1, 16)])\n",
    "avg_val_loss_umpf_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6958ea",
   "metadata": {},
   "source": [
    "# average validation loss on unmapped features in EHR1 using model1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e300d654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.101517</td>\n",
       "      <td>7.635693</td>\n",
       "      <td>7.959547</td>\n",
       "      <td>9.442474</td>\n",
       "      <td>8.099477</td>\n",
       "      <td>8.116611</td>\n",
       "      <td>9.297311</td>\n",
       "      <td>9.788095</td>\n",
       "      <td>9.765993</td>\n",
       "      <td>9.19864</td>\n",
       "      <td>9.764281</td>\n",
       "      <td>9.684067</td>\n",
       "      <td>8.797012</td>\n",
       "      <td>9.207513</td>\n",
       "      <td>9.466866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      umpf1     umpf2     umpf3     umpf4     umpf5     umpf6     umpf7  \\\n",
       "0  8.101517  7.635693  7.959547  9.442474  8.099477  8.116611  9.297311   \n",
       "\n",
       "      umpf8     umpf9   umpf10    umpf11    umpf12    umpf13    umpf14  \\\n",
       "0  9.788095  9.765993  9.19864  9.764281  9.684067  8.797012  9.207513   \n",
       "\n",
       "     umpf15  \n",
       "0  9.466866  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_test_loss_1_mean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca2628",
   "metadata": {},
   "source": [
    "# average validation loss on unmapped features in EHR2 using model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "93ff4975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.387987</td>\n",
       "      <td>9.592985</td>\n",
       "      <td>4.461771</td>\n",
       "      <td>9.18687</td>\n",
       "      <td>8.233776</td>\n",
       "      <td>3.936945</td>\n",
       "      <td>7.684729</td>\n",
       "      <td>8.505713</td>\n",
       "      <td>7.277887</td>\n",
       "      <td>4.779131</td>\n",
       "      <td>6.207377</td>\n",
       "      <td>4.652694</td>\n",
       "      <td>9.696338</td>\n",
       "      <td>4.269492</td>\n",
       "      <td>9.885467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      umpf1     umpf2     umpf3    umpf4     umpf5     umpf6     umpf7  \\\n",
       "0  7.387987  9.592985  4.461771  9.18687  8.233776  3.936945  7.684729   \n",
       "\n",
       "      umpf8     umpf9    umpf10    umpf11    umpf12    umpf13    umpf14  \\\n",
       "0  8.505713  7.277887  4.779131  6.207377  4.652694  9.696338  4.269492   \n",
       "\n",
       "     umpf15  \n",
       "0  9.885467  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_val_loss_umpf_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ecdde4",
   "metadata": {},
   "source": [
    "# average validation loss on unmapped features in EHR2 using model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "01e68658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.259842</td>\n",
       "      <td>9.859417</td>\n",
       "      <td>8.137906</td>\n",
       "      <td>9.765788</td>\n",
       "      <td>9.47144</td>\n",
       "      <td>7.583542</td>\n",
       "      <td>9.328816</td>\n",
       "      <td>9.597952</td>\n",
       "      <td>9.267031</td>\n",
       "      <td>8.16348</td>\n",
       "      <td>8.809281</td>\n",
       "      <td>8.174178</td>\n",
       "      <td>9.876471</td>\n",
       "      <td>8.020665</td>\n",
       "      <td>9.889639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      umpf1     umpf2     umpf3     umpf4    umpf5     umpf6     umpf7  \\\n",
       "0  9.259842  9.859417  8.137906  9.765788  9.47144  7.583542  9.328816   \n",
       "\n",
       "      umpf8     umpf9   umpf10    umpf11    umpf12    umpf13    umpf14  \\\n",
       "0  9.597952  9.267031  8.16348  8.809281  8.174178  9.876471  8.020665   \n",
       "\n",
       "     umpf15  \n",
       "0  9.889639  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_test_loss_2_mean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c65b2a",
   "metadata": {},
   "source": [
    "# Train the final model_2 using all above 90% training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "27112fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "final_train_dataloader_1 = DataLoader(dataset_1_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = Seq2seq_model_pass_final_hs_each_time(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_1, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./each_umpf_model_2_param/EHR1/umpf_\"+str(umpf_id+1)+\"_EHR1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bc6decae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "final_train_dataloader_2 = DataLoader(dataset_2_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = Seq2seq_model_pass_final_hs_each_time(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_2, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./each_umpf_model_2_param/EHR2/umpf_\"+str(umpf_id+1)+\"_EHR2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e1f04",
   "metadata": {},
   "source": [
    "# Test final trained model on 10% test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "15182ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shutingcui/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "models_2_list_ump_EHR1 = []\n",
    "models_2_list_ump_EHR2 = []\n",
    "for i in range(1, 16):\n",
    "    cur_model_1_pth = \"./each_umpf_model_2_param/EHR1/umpf_\" + str(i) + \"_EHR1.pth\"\n",
    "    cur_model_2_pth = \"./each_umpf_model_2_param/EHR2/umpf_\" + str(i) + \"_EHR2.pth\"\n",
    "    \n",
    "    cur_model_1 = Seq2seq_model_pass_final_hs_each_time(i-1, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_1.load_state_dict(torch.load(cur_model_1_pth))\n",
    "    \n",
    "    cur_model_2 = Seq2seq_model_pass_final_hs_each_time(i-1, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_2.load_state_dict(torch.load(cur_model_2_pth))\n",
    "    \n",
    "    models_2_list_ump_EHR1.append(cur_model_1)\n",
    "    models_2_list_ump_EHR2.append(cur_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434c1c2",
   "metadata": {},
   "source": [
    "# Compute [num_of_ump, num_of_mp*sel_len] matrix for both EHR1 and EHR2, with each row calculated as the average predicted matched features' values for current unmapped feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8ca1f254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C15], R2: [C3], R3: [C11], R4: [C4], R5: [C10], R6: [C12], R7: [C7], R8: [C2], R9: [C1], R10: [C9], R11: [C14], R12: [C8], R13: [C5], R14: [C6], R15: [C13]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R15], C2: [R3], C3: [R11], C4: [R4], C5: [R10], C6: [R12], C7: [R7], C8: [R2], C9: [R1], C10: [R9], C11: [R14], C12: [R8], C13: [R5], C14: [R6], C15: [R13]}\n"
     ]
    }
   ],
   "source": [
    "num_ump_f_1 = 15\n",
    "num_ump_f_2 = 15\n",
    "num_mp_f = 5\n",
    "seq_len = 24\n",
    "ump_mp_matrix_1_model_2 = np.zeros((num_ump_f_1, seq_len * num_mp_f))\n",
    "ump_mp_matrix_2_model_2 = np.zeros((num_ump_f_2, seq_len * num_mp_f))\n",
    "for ump_id_1 in range(num_ump_f_1):\n",
    "    cur_model_1 = models_2_list_ump_EHR1[ump_id_1]\n",
    "    cur_model_1.eval()\n",
    "    for data in dataloader_1_test:\n",
    "        cur_pred_map = cur_model_1(data)[\"predicts\"] # (10, 24, 5)\n",
    "#         print(cur_pred_map.detach().numpy().shape)\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0) # [120]\n",
    "        ump_mp_matrix_1_model_2[ump_id_1][:] += cur_pred_map_batch_sum\n",
    "\n",
    "for ump_id_2 in range(num_ump_f_2):\n",
    "    cur_model_2 = models_2_list_ump_EHR2[ump_id_2]\n",
    "    cur_model_2.eval()\n",
    "    for data in dataloader_2_test:\n",
    "        cur_pred_map = cur_model_2(data)[\"predicts\"]\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0)\n",
    "        ump_mp_matrix_2_model_2[ump_id_2][:] += cur_pred_map_batch_sum\n",
    "ump_mp_matrix_1_model_2 = ump_mp_matrix_1_model_2 / patients_test_1\n",
    "ump_mp_matrix_2_model_2 = ump_mp_matrix_2_model_2 / patients_test_2\n",
    "\n",
    "cos_sim_matrix_1_to_2_model_2 = pairwise.cosine_similarity(ump_mp_matrix_1_model_2, ump_mp_matrix_2_model_2)\n",
    "cos_sim_matrix_2_to_1_model_2 = pairwise.cosine_similarity(ump_mp_matrix_2_model_2, ump_mp_matrix_1_model_2)\n",
    "correct_with_match_from_x1_test, correct_with_match_from_x2_test, x1_match_matrix_test, x2_match_matrix_test = Matching_via_HRM(cos_sim_matrix_1_to_2_model_2, cos_sim_matrix_2_to_1_model_2, p, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "50bfdf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(correct_with_match_from_x1_test)\n",
    "print(correct_with_match_from_x2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "64086e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(p, x1_match_matrix_test, x2_match_matrix_test):\n",
    "    TP_x1 = 0\n",
    "    FP_x1 = 0\n",
    "    TN_x1 = 0\n",
    "    FN_x1 = 0\n",
    "    for i in range(p.shape[0]):\n",
    "        for j in range(p.shape[1]):\n",
    "            if (p[i, j] == 1) & (x1_match_matrix_test[i, j] == 1):\n",
    "                TP_x1 = TP_x1 + 1\n",
    "            elif (p[i, j] == 1) & (x1_match_matrix_test[i, j] == 0):\n",
    "                FN_x1 = FN_x1 + 1\n",
    "            elif (p[i, j] == 0) & (x1_match_matrix_test[i, j] == 0):\n",
    "                TN_x1 = TN_x1 + 1\n",
    "            elif (p[i, j] == 0) & (x1_match_matrix_test[i, j] == 1):\n",
    "                FP_x1 = FP_x1 + 1\n",
    "\n",
    "    TP_x2 = 0\n",
    "    FP_x2 = 0\n",
    "    TN_x2 = 0\n",
    "    FN_x2 = 0\n",
    "    for i in range(p.shape[0]):\n",
    "        for j in range(p.shape[1]):\n",
    "            if (p[i, j] == 1) & (x2_match_matrix_test[i, j] == 1):\n",
    "                TP_x2 = TP_x2 + 1\n",
    "            elif (p[i, j] == 1) & (x2_match_matrix_test[i, j] == 0):\n",
    "                FN_x2 = FN_x2 + 1\n",
    "            elif (p[i, j] == 0) & (x2_match_matrix_test[i, j] == 0):\n",
    "                TN_x2 = TN_x2 + 1\n",
    "            elif (p[i, j] == 0) & (x2_match_matrix_test[i, j] == 1):\n",
    "                FP_x2 = FP_x2 + 1\n",
    "    F1_fromx1 = (2 * TP_x1) / (2 * TP_x1 + FN_x1 + FP_x1)\n",
    "    F1_fromx2 = (2 * TP_x2) / (2 * TP_x2 + FN_x2 + FP_x2)\n",
    "\n",
    "    print(\"Sim cor F values \", F1_fromx1, F1_fromx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8e3d5478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim cor F values  0.06666666666666667 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "F1_score(p, x1_match_matrix_test, x2_match_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255a902",
   "metadata": {},
   "source": [
    "# Ablation study 1: Add more RNN layers(tested on 3 stacking layers first) based on model2's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1c8b1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ablation_model_more_rnn_layers(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(ablation_model_more_rnn_layers, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(3, self.batch_size, hidden_dim) # 3 is the number of stacked layers\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, num_layers=3, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.MLP_drop1 = nn.Dropout(p=0.2)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.MLP_drop2 = nn.Dropout(p=0.2)\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.MLP_drop3 = nn.Dropout(p=0.2)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop1(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop2(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop3(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "abb95f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validation model\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "input_dim = 1 # each unmapped feature\n",
    "hidden_dim = 20 # hidden dim of RNN model\n",
    "num_ump_f = 15\n",
    "seq_len = 24\n",
    "num_mp_f = 5\n",
    "lr = 0.002\n",
    "k_folds = 5\n",
    "splits=KFold(n_splits=k_folds, shuffle=True,random_state=42)\n",
    "kfold_test_loss_1 = np.zeros((num_ump_f, k_folds))\n",
    "kfold_test_loss_2 = np.zeros((num_ump_f, k_folds))\n",
    "\n",
    "# trained on unmapped features in EHR1\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_1_kfold)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset_1_kfold, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset_1_kfold, batch_size=batch_size, sampler=test_sampler)\n",
    "    for ump_f_id in range(num_ump_f):\n",
    "        model = ablation_model_more_rnn_layers(ump_f_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        cur_sum_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader, model, optimizer, batch_size)\n",
    "            cur_sum_loss += val(test_loader, model, batch_size)\n",
    "            \n",
    "        kfold_test_loss_1[ump_f_id, fold] = cur_sum_loss / num_epochs\n",
    "        \n",
    "# trained on unmapped features in EHR2\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_2_kfold)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset_2_kfold, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset_2_kfold, batch_size=batch_size, sampler=test_sampler)\n",
    "    for ump_f_id in range(num_ump_f):\n",
    "        model = ablation_model_more_rnn_layers(ump_f_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        cur_sum_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader, model, optimizer, batch_size)\n",
    "            cur_sum_loss += val(test_loader, model, batch_size)\n",
    "            \n",
    "        kfold_test_loss_2[ump_f_id, fold] = cur_sum_loss / num_epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "91f7492a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.567791</td>\n",
       "      <td>3.56303</td>\n",
       "      <td>4.688455</td>\n",
       "      <td>8.337237</td>\n",
       "      <td>5.214758</td>\n",
       "      <td>4.984651</td>\n",
       "      <td>7.759979</td>\n",
       "      <td>9.783238</td>\n",
       "      <td>9.567906</td>\n",
       "      <td>7.515486</td>\n",
       "      <td>9.561841</td>\n",
       "      <td>9.157829</td>\n",
       "      <td>6.315001</td>\n",
       "      <td>7.505744</td>\n",
       "      <td>8.340211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      umpf1    umpf2     umpf3     umpf4     umpf5     umpf6     umpf7  \\\n",
       "0  4.567791  3.56303  4.688455  8.337237  5.214758  4.984651  7.759979   \n",
       "\n",
       "      umpf8     umpf9    umpf10    umpf11    umpf12    umpf13    umpf14  \\\n",
       "0  9.783238  9.567906  7.515486  9.561841  9.157829  6.315001  7.505744   \n",
       "\n",
       "     umpf15  \n",
       "0  8.340211  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_test_loss_1_mean_abla_model = np.mean(kfold_test_loss_1, axis=1)\n",
    "kfold_test_loss_1_mean_df_abla_model = pd.DataFrame(data=kfold_test_loss_1_mean_abla_model.reshape(1, 15), columns=[\"umpf\"+str(i) for i in range(1, 16)])\n",
    "kfold_test_loss_1_mean_df_abla_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "68124fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.51338</td>\n",
       "      <td>9.627464</td>\n",
       "      <td>4.565676</td>\n",
       "      <td>9.239512</td>\n",
       "      <td>8.309349</td>\n",
       "      <td>3.907383</td>\n",
       "      <td>7.682642</td>\n",
       "      <td>8.379988</td>\n",
       "      <td>7.320166</td>\n",
       "      <td>4.776638</td>\n",
       "      <td>6.406842</td>\n",
       "      <td>4.906768</td>\n",
       "      <td>9.710076</td>\n",
       "      <td>4.313153</td>\n",
       "      <td>9.883927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     umpf1     umpf2     umpf3     umpf4     umpf5     umpf6     umpf7  \\\n",
       "0  7.51338  9.627464  4.565676  9.239512  8.309349  3.907383  7.682642   \n",
       "\n",
       "      umpf8     umpf9    umpf10    umpf11    umpf12    umpf13    umpf14  \\\n",
       "0  8.379988  7.320166  4.776638  6.406842  4.906768  9.710076  4.313153   \n",
       "\n",
       "     umpf15  \n",
       "0  9.883927  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_test_loss_2_mean_abla_model = np.mean(kfold_test_loss_2, axis=1)\n",
    "kfold_test_loss_2_mean_df_abla_model = pd.DataFrame(data=kfold_test_loss_2_mean_abla_model.reshape(1, 15), columns=[\"umpf\"+str(i) for i in range(1, 16)])\n",
    "kfold_test_loss_2_mean_df_abla_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "327fb04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "# train final model using all the 90% dataset\n",
    "final_train_dataloader_1_abla_m1 = DataLoader(dataset_1_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = ablation_model_more_rnn_layers(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_1_abla_m1, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./ablation_model_params/Ablation_model_1/EHR1/\"+str(umpf_id+1)+\"_EHR1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ead1e553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "# train final model using all the 90% dataset\n",
    "final_train_dataloader_2_abla_m1 = DataLoader(dataset_2_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = ablation_model_more_rnn_layers(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_2_abla_m1, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./ablation_model_params/Ablation_model_1/EHR2/\"+str(umpf_id+1)+\"_EHR2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7414c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C4], R2: [C10], R3: [C8], R4: [C3], R5: [C12], R6: [C2], R7: [C7], R8: [C15], R9: [C14], R10: [C1], R11: [C6], R12: [C11], R13: [C5], R14: [C13], R15: [C9]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R4], C2: [R10], C3: [R8], C4: [R3], C5: [R12], C6: [R2], C7: [R7], C8: [R15], C9: [R14], C10: [R1], C11: [R6], C12: [R11], C13: [R5], C14: [R13], C15: [R9]}\n"
     ]
    }
   ],
   "source": [
    "# load trained models\n",
    "# models_2_list_ump_EHR1 = []\n",
    "# models_2_list_ump_EHR2 = []\n",
    "abla_1_models_list_EHR1 = []\n",
    "abla_1_models_list_EHR2 = []\n",
    "for i in range(1, 16):\n",
    "    cur_model_1_pth = \"./ablation_model_params/Ablation_model_1/EHR1/\" + str(i) + \"_EHR1.pth\"\n",
    "    cur_model_2_pth = \"./ablation_model_params/Ablation_model_1/EHR2/\" + str(i) + \"_EHR2.pth\"\n",
    "    \n",
    "    cur_model_1 = ablation_model_more_rnn_layers(i-1, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_1.load_state_dict(torch.load(cur_model_1_pth))\n",
    "    \n",
    "    cur_model_2 = ablation_model_more_rnn_layers(i-1, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_2.load_state_dict(torch.load(cur_model_2_pth))\n",
    "    \n",
    "    abla_1_models_list_EHR1.append(cur_model_1)\n",
    "    abla_1_models_list_EHR2.append(cur_model_2)\n",
    "    \n",
    "# Get the avg_cos_sim_matrix based on ump_map_matrix using the predicted mapped features' values from the ump features\n",
    "num_ump_f_1 = 15\n",
    "num_ump_f_2 = 15\n",
    "num_mp_f = 5\n",
    "seq_len = 24\n",
    "ump_mp_matrix_1_abla_1= np.zeros((num_ump_f_1, seq_len * num_mp_f))\n",
    "ump_mp_matrix_2_abla_1 = np.zeros((num_ump_f_2, seq_len * num_mp_f))\n",
    "for ump_id_1 in range(num_ump_f_1):\n",
    "    cur_model_1 = abla_1_models_list_EHR1[ump_id_1]\n",
    "    cur_model_1.eval()\n",
    "    for data in dataloader_1_test:\n",
    "        cur_pred_map = cur_model_1(data)[\"predicts\"] # (10, 24, 5)\n",
    "#         print(cur_pred_map.detach().numpy().shape)\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0) # [120]\n",
    "        ump_mp_matrix_1_abla_1[ump_id_1][:] += cur_pred_map_batch_sum\n",
    "\n",
    "for ump_id_2 in range(num_ump_f_2):\n",
    "    cur_model_2 = abla_1_models_list_EHR2[ump_id_2]\n",
    "    cur_model_2.eval()\n",
    "    for data in dataloader_2_test:\n",
    "        cur_pred_map = cur_model_2(data)[\"predicts\"]\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0)\n",
    "        ump_mp_matrix_2_abla_1[ump_id_2][:] += cur_pred_map_batch_sum\n",
    "\n",
    "ump_mp_matrix_1_abla_1 = ump_mp_matrix_1_abla_1 / patients_test_1\n",
    "ump_mp_matrix_2_abla_1 = ump_mp_matrix_2_abla_1 / patients_test_2\n",
    "\n",
    "cos_sim_matrix_1_to_2_abla_1 = pairwise.cosine_similarity(ump_mp_matrix_1_abla_1, ump_mp_matrix_2_abla_1)\n",
    "cos_sim_matrix_2_to_1_abla_1 = pairwise.cosine_similarity(ump_mp_matrix_2_abla_1, ump_mp_matrix_1_abla_1)\n",
    "correct_with_match_from_x1_test, correct_with_match_from_x2_test, x1_match_matrix_test, x2_match_matrix_test = Matching_via_HRM(cos_sim_matrix_1_to_2_abla_1, cos_sim_matrix_2_to_1_abla_1, p, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c754408f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim cor F values  0.26666666666666666 0.26666666666666666\n"
     ]
    }
   ],
   "source": [
    "F1_score(p, x1_match_matrix_test, x2_match_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bebabb",
   "metadata": {},
   "source": [
    "# Ablation study 2: Only add one Dropout layer after the third linear layer (the layer before the last linear layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "28f1bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ablation_model_only_one_dp(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(ablation_model_only_one_dp, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(1, self.batch_size, hidden_dim)\n",
    "        # RNN to get final hidden states\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        # MLP to predict pre-mapped features(flatten to shape: seq_len x num_mp_f)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.MLP_drop3 = nn.Dropout(p=0.2)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop3(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "68a87a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validation model\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "input_dim = 1 # each unmapped feature\n",
    "hidden_dim = 20 # hidden dim of RNN model\n",
    "num_ump_f = 15\n",
    "seq_len = 24\n",
    "num_mp_f = 5\n",
    "lr = 0.002\n",
    "k_folds = 5\n",
    "splits=KFold(n_splits=k_folds, shuffle=True,random_state=42)\n",
    "kfold_test_loss_1 = np.zeros((num_ump_f, k_folds))\n",
    "kfold_test_loss_2 = np.zeros((num_ump_f, k_folds))\n",
    "\n",
    "# trained on unmapped features in EHR1\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_1_kfold)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset_1_kfold, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset_1_kfold, batch_size=batch_size, sampler=test_sampler)\n",
    "    for ump_f_id in range(num_ump_f):\n",
    "        model = ablation_model_only_one_dp(ump_f_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        cur_sum_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader, model, optimizer, batch_size)\n",
    "            cur_sum_loss += val(test_loader, model, batch_size)\n",
    "            \n",
    "        kfold_test_loss_1[ump_f_id, fold] = cur_sum_loss / num_epochs\n",
    "        \n",
    "# trained on unmapped features in EHR2\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_2_kfold)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset_2_kfold, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset_2_kfold, batch_size=batch_size, sampler=test_sampler)\n",
    "    for ump_f_id in range(num_ump_f):\n",
    "        model = ablation_model_only_one_dp(ump_f_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        cur_sum_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader, model, optimizer, batch_size)\n",
    "            cur_sum_loss += val(test_loader, model, batch_size)\n",
    "            \n",
    "        kfold_test_loss_2[ump_f_id, fold] = cur_sum_loss / num_epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c37d2d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.46891</td>\n",
       "      <td>3.812273</td>\n",
       "      <td>5.600725</td>\n",
       "      <td>8.156265</td>\n",
       "      <td>4.976056</td>\n",
       "      <td>4.797884</td>\n",
       "      <td>7.573601</td>\n",
       "      <td>9.782285</td>\n",
       "      <td>9.550715</td>\n",
       "      <td>7.408799</td>\n",
       "      <td>9.547081</td>\n",
       "      <td>9.119084</td>\n",
       "      <td>6.243166</td>\n",
       "      <td>7.425105</td>\n",
       "      <td>8.414688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     umpf1     umpf2     umpf3     umpf4     umpf5     umpf6     umpf7  \\\n",
       "0  4.46891  3.812273  5.600725  8.156265  4.976056  4.797884  7.573601   \n",
       "\n",
       "      umpf8     umpf9    umpf10    umpf11    umpf12    umpf13    umpf14  \\\n",
       "0  9.782285  9.550715  7.408799  9.547081  9.119084  6.243166  7.425105   \n",
       "\n",
       "     umpf15  \n",
       "0  8.414688  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_test_loss_1_mean_abla_model = np.mean(kfold_test_loss_1, axis=1)\n",
    "kfold_test_loss_1_mean_df_abla_model = pd.DataFrame(data=kfold_test_loss_1_mean_abla_model.reshape(1, 15), columns=[\"umpf\"+str(i) for i in range(1, 16)])\n",
    "kfold_test_loss_1_mean_df_abla_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "53570cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.606606</td>\n",
       "      <td>9.608952</td>\n",
       "      <td>4.685113</td>\n",
       "      <td>9.179137</td>\n",
       "      <td>8.446557</td>\n",
       "      <td>4.305447</td>\n",
       "      <td>7.702867</td>\n",
       "      <td>8.406574</td>\n",
       "      <td>7.365686</td>\n",
       "      <td>4.755599</td>\n",
       "      <td>6.157526</td>\n",
       "      <td>5.095134</td>\n",
       "      <td>9.690174</td>\n",
       "      <td>4.667945</td>\n",
       "      <td>9.88554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      umpf1     umpf2     umpf3     umpf4     umpf5     umpf6     umpf7  \\\n",
       "0  7.606606  9.608952  4.685113  9.179137  8.446557  4.305447  7.702867   \n",
       "\n",
       "      umpf8     umpf9    umpf10    umpf11    umpf12    umpf13    umpf14  \\\n",
       "0  8.406574  7.365686  4.755599  6.157526  5.095134  9.690174  4.667945   \n",
       "\n",
       "    umpf15  \n",
       "0  9.88554  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_test_loss_2_mean_abla_model = np.mean(kfold_test_loss_2, axis=1)\n",
    "kfold_test_loss_2_mean_df_abla_model = pd.DataFrame(data=kfold_test_loss_2_mean_abla_model.reshape(1, 15), columns=[\"umpf\"+str(i) for i in range(1, 16)])\n",
    "kfold_test_loss_2_mean_df_abla_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "990a610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "# train the final model using all 90% dataset\n",
    "final_train_dataloader_1_abla_m2 = DataLoader(dataset_1_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = ablation_model_only_one_dp(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_1_abla_m2, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./ablation_model_params/Ablation_model_2/EHR1/\"+str(umpf_id+1)+\"_EHR1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2b216cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "# train the final model using all 90% dataset\n",
    "final_train_dataloader_2_abla_m2 = DataLoader(dataset_2_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = ablation_model_only_one_dp(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_2_abla_m2, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./ablation_model_params/Ablation_model_2/EHR2/\"+str(umpf_id+1)+\"_EHR2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d9a5375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C12], R2: [C10], R3: [C9], R4: [C8], R5: [C5], R6: [C6], R7: [C7], R8: [C15], R9: [C2], R10: [C1], R11: [C4], R12: [C13], R13: [C11], R14: [C3], R15: [C14]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R12], C2: [R10], C3: [R9], C4: [R8], C5: [R5], C6: [R6], C7: [R7], C8: [R15], C9: [R2], C10: [R1], C11: [R4], C12: [R13], C13: [R11], C14: [R3], C15: [R14]}\n"
     ]
    }
   ],
   "source": [
    "abla_2_models_list_EHR1 = []\n",
    "abla_2_models_list_EHR2 = []\n",
    "for i in range(1, 16):\n",
    "    cur_model_1_pth = \"./ablation_model_params/Ablation_model_2/EHR1/\" + str(i) + \"_EHR1.pth\"\n",
    "    cur_model_2_pth = \"./ablation_model_params/Ablation_model_2/EHR2/\" + str(i) + \"_EHR2.pth\"\n",
    "    \n",
    "    cur_model_1 = ablation_model_only_one_dp(i-1, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_1.load_state_dict(torch.load(cur_model_1_pth))\n",
    "    \n",
    "    cur_model_2 = ablation_model_only_one_dp(i-1, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_2.load_state_dict(torch.load(cur_model_2_pth))\n",
    "    \n",
    "    abla_2_models_list_EHR1.append(cur_model_1)\n",
    "    abla_2_models_list_EHR2.append(cur_model_2)\n",
    "    \n",
    "# Get the avg_cos_sim_matrix based on ump_map_matrix using the predicted mapped features' values from the ump features\n",
    "num_ump_f_1 = 15\n",
    "num_ump_f_2 = 15\n",
    "num_mp_f = 5\n",
    "seq_len = 24\n",
    "ump_mp_matrix_1_abla_2= np.zeros((num_ump_f_1, seq_len * num_mp_f))\n",
    "ump_mp_matrix_2_abla_2 = np.zeros((num_ump_f_2, seq_len * num_mp_f))\n",
    "for ump_id_1 in range(num_ump_f_1):\n",
    "    cur_model_1 = abla_2_models_list_EHR1[ump_id_1]\n",
    "    cur_model_1.eval()\n",
    "    for data in dataloader_1_test:\n",
    "        cur_pred_map = cur_model_1(data)[\"predicts\"] # (10, 24, 5)\n",
    "#         print(cur_pred_map.detach().numpy().shape)\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0) # [120]\n",
    "        ump_mp_matrix_1_abla_2[ump_id_1][:] += cur_pred_map_batch_sum\n",
    "\n",
    "for ump_id_2 in range(num_ump_f_2):\n",
    "    cur_model_2 = abla_2_models_list_EHR2[ump_id_2]\n",
    "    cur_model_2.eval()\n",
    "    for data in dataloader_2_test:\n",
    "        cur_pred_map = cur_model_2(data)[\"predicts\"]\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0)\n",
    "        ump_mp_matrix_2_abla_2[ump_id_2][:] += cur_pred_map_batch_sum\n",
    "\n",
    "ump_mp_matrix_1_abla_2 = ump_mp_matrix_1_abla_2 / patients_test_1\n",
    "ump_mp_matrix_2_abla_2 = ump_mp_matrix_2_abla_2 / patients_test_2\n",
    "\n",
    "cos_sim_matrix_1_to_2_abla_2 = pairwise.cosine_similarity(ump_mp_matrix_1_abla_2, ump_mp_matrix_2_abla_2)\n",
    "cos_sim_matrix_2_to_1_abla_2 = pairwise.cosine_similarity(ump_mp_matrix_2_abla_2, ump_mp_matrix_1_abla_2)\n",
    "correct_with_match_from_x1_test, correct_with_match_from_x2_test, x1_match_matrix_test, x2_match_matrix_test = Matching_via_HRM(cos_sim_matrix_1_to_2_abla_2, cos_sim_matrix_2_to_1_abla_2, p, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9a5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4bcd0dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim cor F values  0.3333333333333333 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "F1_score(p, x1_match_matrix_test, x2_match_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ebaed",
   "metadata": {},
   "source": [
    "# Ablation study 3: Delete all the Dropout layers (only keep ReLU activation function) based on model2's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cc4fa79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ablation_model_no_dp_layers(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(ablation_model_no_dp_layers, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(1, self.batch_size, hidden_dim)\n",
    "        # RNN to get final hidden states\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        # MLP to predict pre-mapped features(flatten to shape: seq_len x num_mp_f)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ebb9f6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "# 5fold cross validation on abla model 3\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "input_dim = 1 # each unmapped feature\n",
    "hidden_dim = 20 # hidden dim of RNN model\n",
    "num_ump_f = 15\n",
    "seq_len = 24\n",
    "num_mp_f = 5\n",
    "lr = 0.002\n",
    "k_folds = 5\n",
    "splits=KFold(n_splits=k_folds, shuffle=True,random_state=42)\n",
    "kfold_test_loss_1 = np.zeros((num_ump_f, k_folds))\n",
    "kfold_test_loss_2 = np.zeros((num_ump_f, k_folds))\n",
    "\n",
    "# trained on unmapped features in EHR1\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_1_kfold)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset_1_kfold, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset_1_kfold, batch_size=batch_size, sampler=test_sampler)\n",
    "    for ump_f_id in range(num_ump_f):\n",
    "        model = ablation_model_no_dp_layers(ump_f_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        cur_sum_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader, model, optimizer, batch_size)\n",
    "            cur_sum_loss += val(test_loader, model, batch_size)\n",
    "            \n",
    "        kfold_test_loss_1[ump_f_id, fold] = cur_sum_loss / num_epochs\n",
    "        \n",
    "# trained on unmapped features in EHR2\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_2_kfold)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset_2_kfold, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset_2_kfold, batch_size=batch_size, sampler=test_sampler)\n",
    "    for ump_f_id in range(num_ump_f):\n",
    "        model = ablation_model_no_dp_layers(ump_f_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        cur_sum_loss = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader, model, optimizer, batch_size)\n",
    "            cur_sum_loss += val(test_loader, model, batch_size)\n",
    "            \n",
    "        kfold_test_loss_2[ump_f_id, fold] = cur_sum_loss / num_epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7f654932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.599545</td>\n",
       "      <td>3.976057</td>\n",
       "      <td>5.182454</td>\n",
       "      <td>8.283076</td>\n",
       "      <td>4.997666</td>\n",
       "      <td>4.461489</td>\n",
       "      <td>8.064072</td>\n",
       "      <td>9.782189</td>\n",
       "      <td>9.542177</td>\n",
       "      <td>7.529328</td>\n",
       "      <td>9.538702</td>\n",
       "      <td>9.077019</td>\n",
       "      <td>6.3282</td>\n",
       "      <td>7.452664</td>\n",
       "      <td>8.342861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      umpf1     umpf2     umpf3     umpf4     umpf5     umpf6     umpf7  \\\n",
       "0  4.599545  3.976057  5.182454  8.283076  4.997666  4.461489  8.064072   \n",
       "\n",
       "      umpf8     umpf9    umpf10    umpf11    umpf12  umpf13    umpf14  \\\n",
       "0  9.782189  9.542177  7.529328  9.538702  9.077019  6.3282  7.452664   \n",
       "\n",
       "     umpf15  \n",
       "0  8.342861  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_test_loss_1_mean_abla_model = np.mean(kfold_test_loss_1, axis=1)\n",
    "kfold_test_loss_1_mean_df_abla_model = pd.DataFrame(data=kfold_test_loss_1_mean_abla_model.reshape(1, 15), columns=[\"umpf\"+str(i) for i in range(1, 16)])\n",
    "kfold_test_loss_1_mean_df_abla_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f10b959d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umpf1</th>\n",
       "      <th>umpf2</th>\n",
       "      <th>umpf3</th>\n",
       "      <th>umpf4</th>\n",
       "      <th>umpf5</th>\n",
       "      <th>umpf6</th>\n",
       "      <th>umpf7</th>\n",
       "      <th>umpf8</th>\n",
       "      <th>umpf9</th>\n",
       "      <th>umpf10</th>\n",
       "      <th>umpf11</th>\n",
       "      <th>umpf12</th>\n",
       "      <th>umpf13</th>\n",
       "      <th>umpf14</th>\n",
       "      <th>umpf15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.346642</td>\n",
       "      <td>9.604694</td>\n",
       "      <td>4.308839</td>\n",
       "      <td>9.16393</td>\n",
       "      <td>8.26945</td>\n",
       "      <td>3.350706</td>\n",
       "      <td>7.571636</td>\n",
       "      <td>8.357831</td>\n",
       "      <td>7.291438</td>\n",
       "      <td>5.168404</td>\n",
       "      <td>6.673473</td>\n",
       "      <td>4.971489</td>\n",
       "      <td>9.702716</td>\n",
       "      <td>4.60717</td>\n",
       "      <td>9.884546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      umpf1     umpf2     umpf3    umpf4    umpf5     umpf6     umpf7  \\\n",
       "0  7.346642  9.604694  4.308839  9.16393  8.26945  3.350706  7.571636   \n",
       "\n",
       "      umpf8     umpf9    umpf10    umpf11    umpf12    umpf13   umpf14  \\\n",
       "0  8.357831  7.291438  5.168404  6.673473  4.971489  9.702716  4.60717   \n",
       "\n",
       "     umpf15  \n",
       "0  9.884546  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_test_loss_2_mean_abla_model = np.mean(kfold_test_loss_2, axis=1)\n",
    "kfold_test_loss_2_mean_df_abla_model = pd.DataFrame(data=kfold_test_loss_2_mean_abla_model.reshape(1, 15), columns=[\"umpf\"+str(i) for i in range(1, 16)])\n",
    "kfold_test_loss_2_mean_df_abla_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2939a11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "# train the final model using all 90% dataset: EHR1\n",
    "final_train_dataloader_1_abla_m3 = DataLoader(dataset_1_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = ablation_model_no_dp_layers(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_1_abla_m3, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./ablation_model_params/Ablation_model_3/EHR1/\"+str(umpf_id+1)+\"_EHR1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d579a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "# train the final model using all 90% dataset: EHR2\n",
    "final_train_dataloader_2_abla_m3 = DataLoader(dataset_2_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = ablation_model_no_dp_layers(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_2_abla_m3, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./ablation_model_params/Ablation_model_3/EHR2/\"+str(umpf_id+1)+\"_EHR2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "650123b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C13], R2: [C9], R3: [C15], R4: [C10], R5: [C12], R6: [C14], R7: [C6], R8: [C1], R9: [C11], R10: [C3], R11: [C5], R12: [C2], R13: [C4], R14: [C8], R15: [C7]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R13], C2: [R9], C3: [R15], C4: [R10], C5: [R12], C6: [R14], C7: [R6], C8: [R1], C9: [R11], C10: [R3], C11: [R5], C12: [R2], C13: [R4], C14: [R8], C15: [R7]}\n"
     ]
    }
   ],
   "source": [
    "# test trained model's performance on left 10% data\n",
    "abla_3_models_list_EHR1 = []\n",
    "abla_3_models_list_EHR2 = []\n",
    "for i in range(1, 16):\n",
    "    cur_model_1_pth = \"./ablation_model_params/Ablation_model_3/EHR1/\" + str(i) + \"_EHR1.pth\"\n",
    "    cur_model_2_pth = \"./ablation_model_params/Ablation_model_3/EHR2/\" + str(i) + \"_EHR2.pth\"\n",
    "    \n",
    "    cur_model_1 = ablation_model_no_dp_layers(i-1, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_1.load_state_dict(torch.load(cur_model_1_pth))\n",
    "    \n",
    "    cur_model_2 = ablation_model_no_dp_layers(i-1, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_2.load_state_dict(torch.load(cur_model_2_pth))\n",
    "    \n",
    "    abla_3_models_list_EHR1.append(cur_model_1)\n",
    "    abla_3_models_list_EHR2.append(cur_model_2)\n",
    "    \n",
    "# Get the avg_cos_sim_matrix based on ump_map_matrix using the predicted mapped features' values from the ump features\n",
    "num_ump_f_1 = 15\n",
    "num_ump_f_2 = 15\n",
    "num_mp_f = 5\n",
    "seq_len = 24\n",
    "ump_mp_matrix_1_abla_3= np.zeros((num_ump_f_1, seq_len * num_mp_f))\n",
    "ump_mp_matrix_2_abla_3 = np.zeros((num_ump_f_2, seq_len * num_mp_f))\n",
    "for ump_id_1 in range(num_ump_f_1):\n",
    "    cur_model_1 = abla_3_models_list_EHR1[ump_id_1]\n",
    "    cur_model_1.eval()\n",
    "    for data in dataloader_1_test:\n",
    "        cur_pred_map = cur_model_1(data)[\"predicts\"] # (10, 24, 5)\n",
    "#         print(cur_pred_map.detach().numpy().shape)\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0) # [120]\n",
    "        ump_mp_matrix_1_abla_3[ump_id_1][:] += cur_pred_map_batch_sum\n",
    "\n",
    "for ump_id_2 in range(num_ump_f_2):\n",
    "    cur_model_2 = abla_3_models_list_EHR2[ump_id_2]\n",
    "    cur_model_2.eval()\n",
    "    for data in dataloader_2_test:\n",
    "        cur_pred_map = cur_model_2(data)[\"predicts\"]\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0)\n",
    "        ump_mp_matrix_2_abla_3[ump_id_2][:] += cur_pred_map_batch_sum\n",
    "\n",
    "ump_mp_matrix_1_abla_3 = ump_mp_matrix_1_abla_3 / patients_test_1\n",
    "ump_mp_matrix_2_abla_3 = ump_mp_matrix_2_abla_3 / patients_test_2\n",
    "\n",
    "cos_sim_matrix_1_to_2_abla_3 = pairwise.cosine_similarity(ump_mp_matrix_1_abla_3, ump_mp_matrix_2_abla_3)\n",
    "cos_sim_matrix_2_to_1_abla_3 = pairwise.cosine_similarity(ump_mp_matrix_2_abla_3, ump_mp_matrix_1_abla_3)\n",
    "correct_with_match_from_x1_test, correct_with_match_from_x2_test, x1_match_matrix_test, x2_match_matrix_test = Matching_via_HRM(cos_sim_matrix_1_to_2_abla_3, cos_sim_matrix_2_to_1_abla_3, p, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "293549d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim cor F values  0.06666666666666667 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "F1_score(p, x1_match_matrix_test, x2_match_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d84a0d",
   "metadata": {},
   "source": [
    "# Ablation study 4: change the activation function in the MLP part from ReLU to LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "84cc7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ablation_model_leakyReLU(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(ablation_model_leakyReLU, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(1, self.batch_size, hidden_dim)\n",
    "        # RNN to get final hidden states\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        # MLP to predict pre-mapped features(flatten to shape: seq_len x num_mp_f)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.MLP_drop1 = nn.Dropout(p=0.2)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.MLP_drop2 = nn.Dropout(p=0.2)\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.MLP_drop3 = nn.Dropout(p=0.2)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.LeakyReLU(0.1)(map_predict)\n",
    "        map_predict = self.MLP_drop1(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.LeakyReLU(0.1)(map_predict)\n",
    "        map_predict = self.MLP_drop2(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.LeakyReLU(0.1)(map_predict)\n",
    "        map_predict = self.MLP_drop3(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9afefd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "# train the final model using all 90% dataset: EHR1\n",
    "final_train_dataloader_1_abla_m4 = DataLoader(dataset_1_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = ablation_model_leakyReLU(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_1_abla_m4, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./ablation_model_params/Ablation_model_4/EHR1/\"+str(umpf_id+1)+\"_EHR1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6cc414b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current unmapped feature id:  0\n",
      "current unmapped feature id:  1\n",
      "current unmapped feature id:  2\n",
      "current unmapped feature id:  3\n",
      "current unmapped feature id:  4\n",
      "current unmapped feature id:  5\n",
      "current unmapped feature id:  6\n",
      "current unmapped feature id:  7\n",
      "current unmapped feature id:  8\n",
      "current unmapped feature id:  9\n",
      "current unmapped feature id:  10\n",
      "current unmapped feature id:  11\n",
      "current unmapped feature id:  12\n",
      "current unmapped feature id:  13\n",
      "current unmapped feature id:  14\n"
     ]
    }
   ],
   "source": [
    "# train the final model using all 90% dataset: EHR1\n",
    "final_train_dataloader_2_abla_m4 = DataLoader(dataset_2_kfold, batch_size=batch_size, shuffle=True)\n",
    "for umpf_id in range(num_ump_f):\n",
    "    print(\"current unmapped feature id: \", umpf_id)\n",
    "    cur_model = ablation_model_leakyReLU(umpf_id, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_optimizer = optim.Adam(cur_model.parameters(), lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "#         print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train(final_train_dataloader_2_abla_m4, cur_model, cur_optimizer, batch_size)\n",
    "    \n",
    "    torch.save(cur_model.state_dict(), \"./ablation_model_params/Ablation_model_4/EHR2/\"+str(umpf_id+1)+\"_EHR2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "55f741d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C15], R2: [C14], R3: [C1], R4: [C4], R5: [C11], R6: [C10], R7: [C7], R8: [C6], R9: [C8], R10: [C12], R11: [C13], R12: [C9], R13: [C5], R14: [C3], R15: [C2]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R15], C2: [R14], C3: [R1], C4: [R4], C5: [R11], C6: [R10], C7: [R7], C8: [R6], C9: [R8], C10: [R12], C11: [R13], C12: [R9], C13: [R5], C14: [R3], C15: [R2]}\n"
     ]
    }
   ],
   "source": [
    "# test trained model's performance on left 10% data\n",
    "abla_4_models_list_EHR1 = []\n",
    "abla_4_models_list_EHR2 = []\n",
    "for i in range(1, 16):\n",
    "    cur_model_1_pth = \"./ablation_model_params/Ablation_model_4/EHR1/\" + str(i) + \"_EHR1.pth\"\n",
    "    cur_model_2_pth = \"./ablation_model_params/Ablation_model_4/EHR2/\" + str(i) + \"_EHR2.pth\"\n",
    "    \n",
    "    cur_model_1 = ablation_model_leakyReLU(i-1, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_1.load_state_dict(torch.load(cur_model_1_pth))\n",
    "    \n",
    "    cur_model_2 = ablation_model_leakyReLU(i-1, batch_size, 1, hidden_dim, seq_len, num_mp_f)\n",
    "    cur_model_2.load_state_dict(torch.load(cur_model_2_pth))\n",
    "    \n",
    "    abla_4_models_list_EHR1.append(cur_model_1)\n",
    "    abla_4_models_list_EHR2.append(cur_model_2)\n",
    "    \n",
    "# Get the avg_cos_sim_matrix based on ump_map_matrix using the predicted mapped features' values from the ump features\n",
    "num_ump_f_1 = 15\n",
    "num_ump_f_2 = 15\n",
    "num_mp_f = 5\n",
    "seq_len = 24\n",
    "ump_mp_matrix_1_abla_4= np.zeros((num_ump_f_1, seq_len * num_mp_f))\n",
    "ump_mp_matrix_2_abla_4 = np.zeros((num_ump_f_2, seq_len * num_mp_f))\n",
    "for ump_id_1 in range(num_ump_f_1):\n",
    "    cur_model_1 = abla_4_models_list_EHR1[ump_id_1]\n",
    "    cur_model_1.eval()\n",
    "    for data in dataloader_1_test:\n",
    "        cur_pred_map = cur_model_1(data)[\"predicts\"] # (10, 24, 5)\n",
    "#         print(cur_pred_map.detach().numpy().shape)\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0) # [120]\n",
    "        ump_mp_matrix_1_abla_4[ump_id_1][:] += cur_pred_map_batch_sum\n",
    "\n",
    "for ump_id_2 in range(num_ump_f_2):\n",
    "    cur_model_2 = abla_4_models_list_EHR2[ump_id_2]\n",
    "    cur_model_2.eval()\n",
    "    for data in dataloader_2_test:\n",
    "        cur_pred_map = cur_model_2(data)[\"predicts\"]\n",
    "        cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0)\n",
    "        ump_mp_matrix_2_abla_4[ump_id_2][:] += cur_pred_map_batch_sum\n",
    "\n",
    "ump_mp_matrix_1_abla_4 = ump_mp_matrix_1_abla_4 / patients_test_1\n",
    "ump_mp_matrix_2_abla_4 = ump_mp_matrix_2_abla_4 / patients_test_2\n",
    "\n",
    "cos_sim_matrix_1_to_2_abla_4 = pairwise.cosine_similarity(ump_mp_matrix_1_abla_4, ump_mp_matrix_2_abla_4)\n",
    "cos_sim_matrix_2_to_1_abla_4 = pairwise.cosine_similarity(ump_mp_matrix_2_abla_4, ump_mp_matrix_1_abla_4)\n",
    "correct_with_match_from_x1_test, correct_with_match_from_x2_test, x1_match_matrix_test, x2_match_matrix_test = Matching_via_HRM(cos_sim_matrix_1_to_2_abla_4, cos_sim_matrix_2_to_1_abla_4, p, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "dca8fd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_with_match_from_x1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b19614cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim cor F values  0.2 0.2\n"
     ]
    }
   ],
   "source": [
    "F1_score(p, x1_match_matrix_test, x2_match_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c002ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
