{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d4d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.manual_seed(0) # make sure the initial hidden state of RNN part keeps the same\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "from matching.games import HospitalResident\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b9f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_1</th>\n",
       "      <th>f2_1</th>\n",
       "      <th>f3_1</th>\n",
       "      <th>f4_1</th>\n",
       "      <th>f5_1</th>\n",
       "      <th>f6_1</th>\n",
       "      <th>f7_1</th>\n",
       "      <th>f8_1</th>\n",
       "      <th>f9_1</th>\n",
       "      <th>f10_1</th>\n",
       "      <th>f11_1</th>\n",
       "      <th>f12_1</th>\n",
       "      <th>f13_1</th>\n",
       "      <th>f14_1</th>\n",
       "      <th>f15_1</th>\n",
       "      <th>f16_1</th>\n",
       "      <th>f17_1</th>\n",
       "      <th>f18_1</th>\n",
       "      <th>f19_1</th>\n",
       "      <th>f20_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.696794</td>\n",
       "      <td>-0.621645</td>\n",
       "      <td>-2.350322</td>\n",
       "      <td>0.857568</td>\n",
       "      <td>0.183480</td>\n",
       "      <td>0.929362</td>\n",
       "      <td>1.662710</td>\n",
       "      <td>3.002824</td>\n",
       "      <td>-0.952821</td>\n",
       "      <td>0.780778</td>\n",
       "      <td>-2.149981</td>\n",
       "      <td>-0.096125</td>\n",
       "      <td>0.686564</td>\n",
       "      <td>0.932175</td>\n",
       "      <td>1.035598</td>\n",
       "      <td>0.100794</td>\n",
       "      <td>0.472664</td>\n",
       "      <td>0.961984</td>\n",
       "      <td>-1.581007</td>\n",
       "      <td>0.418345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.490176</td>\n",
       "      <td>-1.369651</td>\n",
       "      <td>-6.381362</td>\n",
       "      <td>2.446793</td>\n",
       "      <td>0.648104</td>\n",
       "      <td>5.127069</td>\n",
       "      <td>3.632748</td>\n",
       "      <td>6.645584</td>\n",
       "      <td>-1.694836</td>\n",
       "      <td>1.961207</td>\n",
       "      <td>-5.393690</td>\n",
       "      <td>-1.725572</td>\n",
       "      <td>1.115977</td>\n",
       "      <td>2.599284</td>\n",
       "      <td>1.849126</td>\n",
       "      <td>0.149453</td>\n",
       "      <td>1.651858</td>\n",
       "      <td>2.380088</td>\n",
       "      <td>-2.989520</td>\n",
       "      <td>1.523643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.195791</td>\n",
       "      <td>-1.803062</td>\n",
       "      <td>-8.885239</td>\n",
       "      <td>1.678346</td>\n",
       "      <td>1.484633</td>\n",
       "      <td>7.669556</td>\n",
       "      <td>5.218378</td>\n",
       "      <td>9.897765</td>\n",
       "      <td>-2.370407</td>\n",
       "      <td>2.332558</td>\n",
       "      <td>-7.979431</td>\n",
       "      <td>-2.123378</td>\n",
       "      <td>1.262524</td>\n",
       "      <td>3.922189</td>\n",
       "      <td>2.477547</td>\n",
       "      <td>1.246901</td>\n",
       "      <td>1.795737</td>\n",
       "      <td>2.639566</td>\n",
       "      <td>-4.948484</td>\n",
       "      <td>1.801274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.943051</td>\n",
       "      <td>0.117324</td>\n",
       "      <td>-11.007327</td>\n",
       "      <td>0.045042</td>\n",
       "      <td>2.026570</td>\n",
       "      <td>7.370177</td>\n",
       "      <td>6.339163</td>\n",
       "      <td>11.226333</td>\n",
       "      <td>-2.778230</td>\n",
       "      <td>3.804610</td>\n",
       "      <td>-8.785526</td>\n",
       "      <td>-1.515712</td>\n",
       "      <td>0.418063</td>\n",
       "      <td>2.784484</td>\n",
       "      <td>2.132019</td>\n",
       "      <td>1.920771</td>\n",
       "      <td>1.803917</td>\n",
       "      <td>2.866873</td>\n",
       "      <td>-4.924506</td>\n",
       "      <td>1.716181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.792690</td>\n",
       "      <td>2.802358</td>\n",
       "      <td>-5.212722</td>\n",
       "      <td>-1.270905</td>\n",
       "      <td>0.897933</td>\n",
       "      <td>4.450223</td>\n",
       "      <td>2.796456</td>\n",
       "      <td>7.543190</td>\n",
       "      <td>-1.245164</td>\n",
       "      <td>1.265860</td>\n",
       "      <td>-4.883092</td>\n",
       "      <td>-0.436753</td>\n",
       "      <td>-0.691122</td>\n",
       "      <td>2.223674</td>\n",
       "      <td>-0.473904</td>\n",
       "      <td>0.735597</td>\n",
       "      <td>1.970611</td>\n",
       "      <td>0.547733</td>\n",
       "      <td>-3.001847</td>\n",
       "      <td>0.299120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       f1_1      f2_1       f3_1      f4_1      f5_1      f6_1      f7_1  \\\n",
       "0  1.696794 -0.621645  -2.350322  0.857568  0.183480  0.929362  1.662710   \n",
       "1  4.490176 -1.369651  -6.381362  2.446793  0.648104  5.127069  3.632748   \n",
       "2  6.195791 -1.803062  -8.885239  1.678346  1.484633  7.669556  5.218378   \n",
       "3  5.943051  0.117324 -11.007327  0.045042  2.026570  7.370177  6.339163   \n",
       "4  1.792690  2.802358  -5.212722 -1.270905  0.897933  4.450223  2.796456   \n",
       "\n",
       "        f8_1      f9_1     f10_1     f11_1     f12_1     f13_1     f14_1  \\\n",
       "0   3.002824 -0.952821  0.780778 -2.149981 -0.096125  0.686564  0.932175   \n",
       "1   6.645584 -1.694836  1.961207 -5.393690 -1.725572  1.115977  2.599284   \n",
       "2   9.897765 -2.370407  2.332558 -7.979431 -2.123378  1.262524  3.922189   \n",
       "3  11.226333 -2.778230  3.804610 -8.785526 -1.515712  0.418063  2.784484   \n",
       "4   7.543190 -1.245164  1.265860 -4.883092 -0.436753 -0.691122  2.223674   \n",
       "\n",
       "      f15_1     f16_1     f17_1     f18_1     f19_1     f20_1  \n",
       "0  1.035598  0.100794  0.472664  0.961984 -1.581007  0.418345  \n",
       "1  1.849126  0.149453  1.651858  2.380088 -2.989520  1.523643  \n",
       "2  2.477547  1.246901  1.795737  2.639566 -4.948484  1.801274  \n",
       "3  2.132019  1.920771  1.803917  2.866873 -4.924506  1.716181  \n",
       "4 -0.473904  0.735597  1.970611  0.547733 -3.001847  0.299120  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_EHR1 = pd.read_csv(\"./data_EHR1.csv\")\n",
    "data_EHR1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ccaa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_EHR1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623b940c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_2</th>\n",
       "      <th>f2_2</th>\n",
       "      <th>f3_2</th>\n",
       "      <th>f4_2</th>\n",
       "      <th>f5_2</th>\n",
       "      <th>f15_2</th>\n",
       "      <th>f14_2</th>\n",
       "      <th>f6_2</th>\n",
       "      <th>f17_2</th>\n",
       "      <th>f9_2</th>\n",
       "      <th>f7_2</th>\n",
       "      <th>f12_2</th>\n",
       "      <th>f20_2</th>\n",
       "      <th>f19_2</th>\n",
       "      <th>f11_2</th>\n",
       "      <th>f18_2</th>\n",
       "      <th>f10_2</th>\n",
       "      <th>f16_2</th>\n",
       "      <th>f8_2</th>\n",
       "      <th>f13_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.002464</td>\n",
       "      <td>-0.054770</td>\n",
       "      <td>-1.607723</td>\n",
       "      <td>0.413153</td>\n",
       "      <td>0.526490</td>\n",
       "      <td>0.808553</td>\n",
       "      <td>0.521133</td>\n",
       "      <td>1.489252</td>\n",
       "      <td>1.093870</td>\n",
       "      <td>-0.143866</td>\n",
       "      <td>1.521181</td>\n",
       "      <td>-1.214374</td>\n",
       "      <td>-0.054487</td>\n",
       "      <td>-0.620618</td>\n",
       "      <td>-0.165420</td>\n",
       "      <td>0.877329</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>-0.041809</td>\n",
       "      <td>2.285389</td>\n",
       "      <td>-0.688339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.208255</td>\n",
       "      <td>-0.430312</td>\n",
       "      <td>3.542310</td>\n",
       "      <td>0.273496</td>\n",
       "      <td>0.523185</td>\n",
       "      <td>1.403259</td>\n",
       "      <td>0.807507</td>\n",
       "      <td>-0.765867</td>\n",
       "      <td>0.872294</td>\n",
       "      <td>0.744647</td>\n",
       "      <td>-0.662414</td>\n",
       "      <td>0.211199</td>\n",
       "      <td>-0.622928</td>\n",
       "      <td>0.033095</td>\n",
       "      <td>1.369378</td>\n",
       "      <td>1.721680</td>\n",
       "      <td>-1.527514</td>\n",
       "      <td>-0.234272</td>\n",
       "      <td>0.200160</td>\n",
       "      <td>-0.147486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.177253</td>\n",
       "      <td>-0.359707</td>\n",
       "      <td>7.928680</td>\n",
       "      <td>0.673136</td>\n",
       "      <td>-0.539660</td>\n",
       "      <td>0.083743</td>\n",
       "      <td>0.467750</td>\n",
       "      <td>-3.756303</td>\n",
       "      <td>-0.516528</td>\n",
       "      <td>1.122243</td>\n",
       "      <td>-4.087978</td>\n",
       "      <td>-0.085167</td>\n",
       "      <td>-1.435108</td>\n",
       "      <td>-0.203887</td>\n",
       "      <td>3.237390</td>\n",
       "      <td>1.130163</td>\n",
       "      <td>-3.310383</td>\n",
       "      <td>-1.228359</td>\n",
       "      <td>-3.529958</td>\n",
       "      <td>0.776517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.293687</td>\n",
       "      <td>-0.708697</td>\n",
       "      <td>6.471826</td>\n",
       "      <td>0.595019</td>\n",
       "      <td>-2.137115</td>\n",
       "      <td>-0.089151</td>\n",
       "      <td>-1.102489</td>\n",
       "      <td>-3.336020</td>\n",
       "      <td>-0.365526</td>\n",
       "      <td>0.826688</td>\n",
       "      <td>-4.684949</td>\n",
       "      <td>-0.328677</td>\n",
       "      <td>-1.541318</td>\n",
       "      <td>-0.260947</td>\n",
       "      <td>2.497054</td>\n",
       "      <td>-0.693162</td>\n",
       "      <td>-2.074722</td>\n",
       "      <td>-1.027206</td>\n",
       "      <td>-3.263632</td>\n",
       "      <td>0.990452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941395</td>\n",
       "      <td>-1.305723</td>\n",
       "      <td>3.126918</td>\n",
       "      <td>-0.427142</td>\n",
       "      <td>-2.480852</td>\n",
       "      <td>-0.253852</td>\n",
       "      <td>-1.456013</td>\n",
       "      <td>-1.420847</td>\n",
       "      <td>0.261801</td>\n",
       "      <td>0.014554</td>\n",
       "      <td>-1.940401</td>\n",
       "      <td>-0.648343</td>\n",
       "      <td>-1.986704</td>\n",
       "      <td>-0.346978</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-1.558701</td>\n",
       "      <td>-0.040872</td>\n",
       "      <td>-0.748870</td>\n",
       "      <td>-0.606107</td>\n",
       "      <td>1.572000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       f1_2      f2_2      f3_2      f4_2      f5_2     f15_2     f14_2  \\\n",
       "0  1.002464 -0.054770 -1.607723  0.413153  0.526490  0.808553  0.521133   \n",
       "1 -0.208255 -0.430312  3.542310  0.273496  0.523185  1.403259  0.807507   \n",
       "2 -3.177253 -0.359707  7.928680  0.673136 -0.539660  0.083743  0.467750   \n",
       "3 -3.293687 -0.708697  6.471826  0.595019 -2.137115 -0.089151 -1.102489   \n",
       "4  0.941395 -1.305723  3.126918 -0.427142 -2.480852 -0.253852 -1.456013   \n",
       "\n",
       "       f6_2     f17_2      f9_2      f7_2     f12_2     f20_2     f19_2  \\\n",
       "0  1.489252  1.093870 -0.143866  1.521181 -1.214374 -0.054487 -0.620618   \n",
       "1 -0.765867  0.872294  0.744647 -0.662414  0.211199 -0.622928  0.033095   \n",
       "2 -3.756303 -0.516528  1.122243 -4.087978 -0.085167 -1.435108 -0.203887   \n",
       "3 -3.336020 -0.365526  0.826688 -4.684949 -0.328677 -1.541318 -0.260947   \n",
       "4 -1.420847  0.261801  0.014554 -1.940401 -0.648343 -1.986704 -0.346978   \n",
       "\n",
       "      f11_2     f18_2     f10_2     f16_2      f8_2     f13_2  \n",
       "0 -0.165420  0.877329  0.684882 -0.041809  2.285389 -0.688339  \n",
       "1  1.369378  1.721680 -1.527514 -0.234272  0.200160 -0.147486  \n",
       "2  3.237390  1.130163 -3.310383 -1.228359 -3.529958  0.776517  \n",
       "3  2.497054 -0.693162 -2.074722 -1.027206 -3.263632  0.990452  \n",
       "4 -0.000081 -1.558701 -0.040872 -0.748870 -0.606107  1.572000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_EHR2 = pd.read_csv(\"./data_EHR2.csv\")\n",
    "data_EHR2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c90fc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_EHR2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3dd20",
   "metadata": {},
   "source": [
    "# Get golden-standard-list from permutation matrix for the unmapped features in 2 EHRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7781ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f6_1', 'f7_1', 'f8_1', 'f9_1', 'f10_1', 'f11_1', 'f12_1', 'f13_1', 'f14_1', 'f15_1', 'f16_1', 'f17_1', 'f18_1', 'f19_1', 'f20_1']\n",
      "['f15_2', 'f14_2', 'f6_2', 'f17_2', 'f9_2', 'f7_2', 'f12_2', 'f20_2', 'f19_2', 'f11_2', 'f18_2', 'f10_2', 'f16_2', 'f8_2', 'f13_2']\n"
     ]
    }
   ],
   "source": [
    "# get permutation matrix for the unmapped features in 2 EHRs\n",
    "ump_f_EHR1 = list(data_EHR1.columns[5:])\n",
    "ump_f_EHR2 = list(data_EHR2.columns[5:])\n",
    "\n",
    "print(ump_f_EHR1)\n",
    "print(ump_f_EHR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5d03f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.zeros((len(ump_f_EHR1), len(ump_f_EHR2)))\n",
    "for i in range(len(ump_f_EHR1)):\n",
    "    for j in range(len(ump_f_EHR2)):\n",
    "        if ump_f_EHR1[i][1:-2] == ump_f_EHR2[j][1:-2]:\n",
    "            p[i][j] = 1\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928975a",
   "metadata": {},
   "source": [
    "# Training seq-to-seq model to regard transformation function as the fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f64b954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer gen_data shape from (60000, 20) to (2500, 24, 20)\n",
    "data_1 = data_EHR1.to_numpy().reshape((2500, 24, 20))\n",
    "data_2 = data_EHR2.to_numpy().reshape((2500, 24, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a874be3",
   "metadata": {},
   "source": [
    "### Define Dataset\n",
    "* input: unmapped feature i in 2 EHRS (totally 15 unmapped features in 2 EHRs, the last 15 columns in data_1/data_2)\n",
    "* target: pre-mapped features in 2 EHRS (the first 5 columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf1182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq_Dataset(Dataset):\n",
    "    def __init__(self, data, EHR_version): \n",
    "        self.patients_num = data.shape[0]\n",
    "        self.ump_features_list = []\n",
    "        self.mp_features_list = []\n",
    "        \n",
    "        # load mp_features\n",
    "        if os.path.exists(\"./mp_f_tensors_\" + str(EHR_version) + \".pt\"):\n",
    "            self.mp_features_list = torch.load(\"./mp_f_tensors_\" + str(EHR_version) + \".pt\")\n",
    "            print(\"Finish loading mapped features' tensors!\")\n",
    "        else:\n",
    "            for i in range(self.patients_num):\n",
    "                cur_mp_features = torch.tensor(data[i,:,:5])\n",
    "                self.mp_features_list.append(cur_mp_features.float())\n",
    "            \n",
    "            torch.save(self.mp_features_list, \"./mp_f_tensors_\" + str(EHR_version) + \".pt\")\n",
    "            print(\"Finish transforming mapped features' tensors!\")\n",
    "        \n",
    "        # load ump_features\n",
    "        if os.path.exists(\"./ump_f_tensors_\" + str(EHR_version) + \".pt\"):\n",
    "            self.ump_features_list = torch.load(\"./ump_f_tensors_\" + str(EHR_version) + \".pt\")\n",
    "            print(\"Finish loading unmapped features' tensors!\")\n",
    "        else:\n",
    "            for i in range(self.patients_num):\n",
    "                cur_ump_features = torch.tensor(data[i, :, 5:])\n",
    "                self.ump_features_list.append(cur_ump_features.float())\n",
    "                \n",
    "            torch.save(self.ump_features_list, \"./ump_f_tensors_\" + str(EHR_version) + \".pt\")\n",
    "            print(\"Finish transforming unmapped features' tensors!\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.patients_num\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_mp_features = self.mp_features_list[idx]\n",
    "        # shape: [24, 5]\n",
    "        sample_ump_features = self.ump_features_list[idx]\n",
    "        # shape: [24, 15]\n",
    "        return sample_mp_features, sample_ump_features\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085277d9",
   "metadata": {},
   "source": [
    "### Define utility functions used in below validation process ( function: val(val_loader, model, batch_size) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5f4fe",
   "metadata": {},
   "source": [
    "* Get_ump_mp_matrix() : get the ump-predicted_mp matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd90de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_ump_mp_matrix(num_ump_f, num_mp_f, seq_len, models_list_ump, val_dataloader):\n",
    "    ump_mp_matrix = np.zeros((num_ump_f, seq_len * num_mp_f))\n",
    "    for ump_id in range(num_ump_f):\n",
    "        cur_model = models_list_ump[ump_id]\n",
    "        cur_model.eval()\n",
    "        for data in val_dataloader:\n",
    "            cur_pred_map = cur_model(data)[\"predicts\"] # (10, 24, 5)\n",
    "            cur_pred_map_batch_sum = np.sum(cur_pred_map.detach().numpy().reshape((10, 120)), axis=0) # [120]\n",
    "            ump_mp_matrix[ump_id][:] += cur_pred_map_batch_sum\n",
    "\n",
    "    patients_num = len(val_dataloader.dataset)\n",
    "    ump_mp_matrix = ump_mp_matrix / patients_num\n",
    "    return ump_mp_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785145e7",
   "metadata": {},
   "source": [
    "* Matching_via_HRM(): Based on above cos_sim_matrix, apply Gale-Shapley Algorithm to get correct_matched number and match_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5dc5de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Matching_via_HRM(C_X1_train, C_X2_train, P_x1_O_to_R, num_mapped_axis):  # in this case here the small feature sized database is X1, so we need to treat it as hospital and there will be capacities on it.\n",
    "    ####### ----------  X1 train ------------- ##########\n",
    "\n",
    "    true_features_pref_X1_train = {}\n",
    "    cross_recon_features_pref_X1_train = {}\n",
    "    capacities_X1_train = {}\n",
    "\n",
    "    for i in range(C_X1_train.shape[0]):  # C_X1_train.shape[0]: number of unmapped features in dataset_1\n",
    "        sorted_index = np.argsort(-C_X1_train[i, :])\n",
    "        sorted_col_index = [\"C\" + str(sorted_index[v] + 1) for v in range(len(sorted_index))]\n",
    "        true_features_pref_X1_train[\"R\" + str(i + 1)] = sorted_col_index\n",
    "        capacities_X1_train[\"R\" + str(i + 1)] = 1\n",
    "\n",
    "    for j in range(C_X1_train.shape[1]): # C_X1_train.shape[1]:  number of unmapped features in dataset_2\n",
    "        sorted_index = np.argsort(-C_X1_train[:, j])\n",
    "        sorted_col_index = [\"R\" + str(sorted_index[v] + 1) for v in range(len(sorted_index))]\n",
    "        cross_recon_features_pref_X1_train[\"C\" + str(j + 1)] = sorted_col_index\n",
    "\n",
    "    game_X1_train = HospitalResident.create_from_dictionaries(cross_recon_features_pref_X1_train,\n",
    "                                                              true_features_pref_X1_train,\n",
    "                                                              capacities_X1_train)\n",
    "\n",
    "    ####### ----------  X2 train ------------- ##########\n",
    "\n",
    "    true_features_pref_X2_train = {}\n",
    "    cross_recon_features_pref_X2_train = {}\n",
    "    capacities_X2_train = {}\n",
    "\n",
    "    for i in range(C_X2_train.shape[0]):  # C_X2_train.shape[0]: number of unmapped features in dataset_2\n",
    "        sorted_index = np.argsort(-C_X2_train[i, :])\n",
    "        sorted_col_index = [\"C\" + str(sorted_index[v] + 1) for v in range(len(sorted_index))]\n",
    "        true_features_pref_X2_train[\"R\" + str(i + 1)] = sorted_col_index\n",
    "\n",
    "    for j in range(C_X2_train.shape[1]):  # C_X2_train.shape[1]: number of unmapped features in dataset_1\n",
    "        sorted_index = np.argsort(-C_X2_train[:, j])\n",
    "        sorted_col_index = [\"R\" + str(sorted_index[v] + 1) for v in range(len(sorted_index))]\n",
    "        cross_recon_features_pref_X2_train[\"C\" + str(j + 1)] = sorted_col_index\n",
    "        capacities_X2_train[\"C\" + str(j + 1)] = 1\n",
    "\n",
    "    # create_from_dictionaries(resident_prefs, hospital_prefs, capacities, clean=False)\n",
    "    game_X2_train = HospitalResident.create_from_dictionaries(true_features_pref_X2_train,\n",
    "                                                              cross_recon_features_pref_X2_train,\n",
    "                                                              capacities_X2_train)\n",
    "\n",
    "       ######   ------------  Final matching -----------   ##########\n",
    "\n",
    "#     print(\"\\n ------- Matching from X1_train  --------- \\n\")\n",
    "    matching_x1_train = game_X1_train.solve()\n",
    "#     print(matching_x1_train)\n",
    "\n",
    "#     print(\"\\n ------- Matching from X2_train  --------- \\n\")\n",
    "    matching_x2_train = game_X2_train.solve()\n",
    "#     print(matching_x2_train)\n",
    "    x1_train_y = [int(str(v[0])[1:]) if v else None for v in matching_x1_train.values()]\n",
    "    x2_train_y = [int(str(v[0])[1:]) if v else None for v in matching_x2_train.values()]\n",
    "\n",
    "    # matching matrices\n",
    "    matching_x1_train_matrix = np.zeros(C_X1_train.shape)\n",
    "    # shape: [num_unmapped_features_in_d1, num_unmapped_features_in_d2]\n",
    "    matching_x2_train_matrix = np.zeros(np.transpose(C_X2_train).shape)\n",
    "    # shape: [num_unmapped_features_in_d1, num_unmapped_features_in_d2]\n",
    "\n",
    "    for i in range(matching_x1_train_matrix.shape[0]):  # number of unmapped features in d_1\n",
    "        if x1_train_y[i] is not None:\n",
    "            matching_x1_train_matrix[i, x1_train_y[i] - 1] = 1  # shape: [# of ump features in d1, # of ump features in d2]\n",
    "        # unmapped feature i in d_1 and unmapped feature \"x1_train_y[i] - 1\" in d_2 has a match\n",
    "\n",
    "    for i in range(matching_x2_train_matrix.shape[0]):  # number of unmapped features in d_1\n",
    "        if x2_train_y[i] is not None:\n",
    "            matching_x2_train_matrix[i, x2_train_y[i] - 1] = 1  # shape: [# of ump features in d1, # of ump features in d2]\n",
    "        # unmapped feature i in d_1 and unmapped feature \"x2_train_y[i] - 1\" in d_2 has a match\n",
    "    # getting the number of correct matches that had a match in other database\n",
    "    num_correct_from_x1 = 0\n",
    "    num_correct_from_x2 = 0\n",
    "    for i in range(P_x1_O_to_R.shape[0]):  # number of unmapped features in d_1\n",
    "        if np.all(P_x1_O_to_R[i] == matching_x1_train_matrix[i]):\n",
    "            # only when the positions of 0-1 are exactly the same, will this condition be true\n",
    "            num_correct_from_x1 = num_correct_from_x1 + 1\n",
    "        if np.all(P_x1_O_to_R[i] == matching_x2_train_matrix[i]):\n",
    "            num_correct_from_x2 = num_correct_from_x2 + 1\n",
    "\n",
    "    return num_correct_from_x1, num_correct_from_x2, matching_x1_train_matrix, matching_x2_train_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342b5e2",
   "metadata": {},
   "source": [
    "* F1_score(): get the performance metric for validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c2420ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(p, x1_match_matrix_test, x2_match_matrix_test):\n",
    "    TP_x1 = 0\n",
    "    FP_x1 = 0\n",
    "    TN_x1 = 0\n",
    "    FN_x1 = 0\n",
    "    for i in range(p.shape[0]):\n",
    "        for j in range(p.shape[1]):\n",
    "            if (p[i, j] == 1) & (x1_match_matrix_test[i, j] == 1):\n",
    "                TP_x1 = TP_x1 + 1\n",
    "            elif (p[i, j] == 1) & (x1_match_matrix_test[i, j] == 0):\n",
    "                FN_x1 = FN_x1 + 1\n",
    "            elif (p[i, j] == 0) & (x1_match_matrix_test[i, j] == 0):\n",
    "                TN_x1 = TN_x1 + 1\n",
    "            elif (p[i, j] == 0) & (x1_match_matrix_test[i, j] == 1):\n",
    "                FP_x1 = FP_x1 + 1\n",
    "\n",
    "    TP_x2 = 0\n",
    "    FP_x2 = 0\n",
    "    TN_x2 = 0\n",
    "    FN_x2 = 0\n",
    "    for i in range(p.shape[0]):\n",
    "        for j in range(p.shape[1]):\n",
    "            if (p[i, j] == 1) & (x2_match_matrix_test[i, j] == 1):\n",
    "                TP_x2 = TP_x2 + 1\n",
    "            elif (p[i, j] == 1) & (x2_match_matrix_test[i, j] == 0):\n",
    "                FN_x2 = FN_x2 + 1\n",
    "            elif (p[i, j] == 0) & (x2_match_matrix_test[i, j] == 0):\n",
    "                TN_x2 = TN_x2 + 1\n",
    "            elif (p[i, j] == 0) & (x2_match_matrix_test[i, j] == 1):\n",
    "                FP_x2 = FP_x2 + 1\n",
    "    F1_fromx1 = (2 * TP_x1) / (2 * TP_x1 + FN_x1 + FP_x1)\n",
    "    F1_fromx2 = (2 * TP_x2) / (2 * TP_x2 + FN_x2 + FP_x2)\n",
    "\n",
    "#     print(\"Sim cor F values \", F1_fromx1, F1_fromx2)\n",
    "    return F1_fromx1, F1_fromx2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f12658",
   "metadata": {},
   "source": [
    "### Define train() val() function in k-fold cross validation:\n",
    "* train()\n",
    "* val(): return the F1 score of predicting matched pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a63b782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer):\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        cur_loss = model(data)[\"loss\"]\n",
    "        cur_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def val(val_loader_1, val_loader_2, num_ump_f_1, num_ump_f_2, num_mp_f, seq_len, models_list_1, models_list_2, true_p):\n",
    "    \n",
    "    ump_mp_matrix_1 = Get_ump_mp_matrix(num_ump_f_1, num_mp_f, seq_len, models_list_1, val_loader_1)\n",
    "    ump_mp_matrix_2 = Get_ump_mp_matrix(num_ump_f_2, num_mp_f, seq_len, models_list_2, val_loader_2)\n",
    "    \n",
    "    cos_sim_matrix_1_to_2 = pairwise.cosine_similarity(ump_mp_matrix_1, ump_mp_matrix_2)\n",
    "    cos_sim_matrix_2_to_1 = pairwise.cosine_similarity(ump_mp_matrix_2, ump_mp_matrix_1)\n",
    "    correct_x1_test, correct_x2_test, x1_match_matrix, x2_match_matrix = Matching_via_HRM(cos_sim_matrix_1_to_2, cos_sim_matrix_2_to_1, true_p, num_mp_f)\n",
    "    f1_fromx1, f1_fromx2 = F1_score(true_p, x1_match_matrix, x2_match_matrix)\n",
    "    \n",
    "    return f1_fromx1, f1_fromx2\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff8159",
   "metadata": {},
   "source": [
    "### Using all the data to apply 5-fold cross validation to get each ablation model's avrage performance, then select the best one to be trained on all the dataset to slightly improved the final model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6a4c601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading mapped features' tensors!\n",
      "Finish loading unmapped features' tensors!\n",
      "Finish loading mapped features' tensors!\n",
      "Finish loading unmapped features' tensors!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "\n",
    "dataset_1_all = Seq2seq_Dataset(data_1, 1)\n",
    "dataset_2_all = Seq2seq_Dataset(data_2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b4dc5",
   "metadata": {},
   "source": [
    "### Create kfold_cv_dl_list_1, kfold_cv_dl_list_2 to store train_idx and val_idx for each fold of 2 EHRs respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4183fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 5\n",
    "splits=KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "kfold_cv_dl_list_1 = []\n",
    "kfold_cv_dl_list_2 = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_1_all)))):\n",
    "    cur_split = {}\n",
    "    cur_split[\"train_idx\"] = train_idx\n",
    "    cur_split[\"val_idx\"] = val_idx\n",
    "    kfold_cv_dl_list_1.append(cur_split)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset_2_all)))):\n",
    "    cur_split = {}\n",
    "    cur_split[\"train_idx\"] = train_idx\n",
    "    cur_split[\"val_idx\"] = val_idx\n",
    "    kfold_cv_dl_list_2.append(cur_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a408294",
   "metadata": {},
   "source": [
    "# Define model_2: feed the output(the final hidden state of the last layer on each time stamp, with shape: [batch_size, seq_len(24), hidden_dim]) to MLP to predict mapped time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b37a2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Seq2seq_model_pass_final_hs_each_time(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(Seq2seq_model_pass_final_hs_each_time, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(1, self.batch_size, hidden_dim)\n",
    "        # RNN to get final hidden states\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        # MLP to predict pre-mapped features(flatten to shape: seq_len x num_mp_f)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.MLP_drop1 = nn.Dropout(p=0.2)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.MLP_drop2 = nn.Dropout(p=0.2)\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.MLP_drop3 = nn.Dropout(p=0.2)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop1(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop2(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop3(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632ff41",
   "metadata": {},
   "source": [
    "# K-fold cross validation on the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36487eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- When hidden_dim equals to:  5 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.10666666666666666  std_f1_fromx1:  0.05333333333333334\n",
      "avg_f1_fromx2:  0.10666666666666666  std_f1_fromx1:  0.05333333333333334\n",
      "\n",
      " ------- When hidden_dim equals to:  10 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.12  std_f1_fromx1:  0.11469767022723501\n",
      "avg_f1_fromx2:  0.12  std_f1_fromx1:  0.11469767022723501\n",
      "\n",
      " ------- When hidden_dim equals to:  15 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.06666666666666668  std_f1_fromx1:  0.05962847939999439\n",
      "avg_f1_fromx2:  0.06666666666666668  std_f1_fromx1:  0.05962847939999439\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "input_dim = 1 # each unmapped feature\n",
    "hidden_dim_list = [5, 10, 15] # hidden dim of RNN model\n",
    "num_ump_f_1 = 15\n",
    "num_ump_f_2 = 15\n",
    "seq_len = 24\n",
    "num_mp_f = 5\n",
    "lr = 0.002\n",
    "\n",
    "for tune_time in range(len(hidden_dim_list)):\n",
    "    hidden_dim = hidden_dim_list[tune_time]\n",
    "    \n",
    "    kfold_results_1_model2 = []\n",
    "    kfold_results_2_model2 = []\n",
    "    for fold, idx_list in enumerate(zip(kfold_cv_dl_list_1, kfold_cv_dl_list_2)):\n",
    "#         print(\"current fold: \", fold)\n",
    "        train_idx_1 = idx_list[0][\"train_idx\"]\n",
    "        val_idx_1 = idx_list[0][\"val_idx\"]\n",
    "        train_idx_2 = idx_list[1][\"train_idx\"]\n",
    "        val_idx_2 = idx_list[1][\"val_idx\"]\n",
    "\n",
    "        train_sampler_1 = SubsetRandomSampler(train_idx_1)\n",
    "        val_sampler_1 = SubsetRandomSampler(val_idx_1)\n",
    "\n",
    "        train_sampler_2 = SubsetRandomSampler(train_idx_2)\n",
    "        val_sampler_2 = SubsetRandomSampler(val_idx_2)\n",
    "\n",
    "        train_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=train_sampler_1)\n",
    "        val_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=val_sampler_1)\n",
    "\n",
    "        train_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=train_sampler_2)\n",
    "        val_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=val_sampler_2)\n",
    "\n",
    "        trained_models_list_1 = []\n",
    "        trained_models_list_2 = []\n",
    "\n",
    "        for ump_id in range(num_ump_f_1):\n",
    "            model = Seq2seq_model_pass_final_hs_each_time(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                train(train_loader_1, model, optimizer)\n",
    "            trained_models_list_1.append(model)\n",
    "\n",
    "        for ump_id in range(num_ump_f_2):\n",
    "            model = Seq2seq_model_pass_final_hs_each_time(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                train(train_loader_2, model, optimizer)\n",
    "            trained_models_list_2.append(model)\n",
    "\n",
    "        f1_fromx1, f1_fromx2 = val(val_loader_1, val_loader_2, num_ump_f_1, num_ump_f_2, num_mp_f, seq_len, trained_models_list_1, trained_models_list_2, p)\n",
    "        kfold_results_1_model2.append(f1_fromx1)\n",
    "        kfold_results_2_model2.append(f1_fromx2)\n",
    "    print(\"\\n ------- When hidden_dim equals to: \", hidden_dim, \"--------- \\n\")\n",
    "    print(\"avg_f1_fromx1: \", np.mean(kfold_results_1_model2), \" std_f1_fromx1: \", np.std(kfold_results_1_model2))\n",
    "    print(\"avg_f1_fromx2: \", np.mean(kfold_results_2_model2), \" std_f1_fromx1: \", np.std(kfold_results_2_model2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b85916",
   "metadata": {},
   "source": [
    "# Ablation study 1: Add more RNN layers(tested on 3 stacking layers first) based on model2's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c97cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ablation_model_more_rnn_layers(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(ablation_model_more_rnn_layers, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(3, self.batch_size, hidden_dim) # 3 is the number of stacked layers\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, num_layers=3, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.MLP_drop1 = nn.Dropout(p=0.2)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.MLP_drop2 = nn.Dropout(p=0.2)\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.MLP_drop3 = nn.Dropout(p=0.2)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop1(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop2(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop3(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "edf3a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  5 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.04  std_f1_fromx1:  0.05333333333333333\n",
      "avg_f1_fromx2:  0.04  std_f1_fromx1:  0.05333333333333333\n",
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  10 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.08  std_f1_fromx1:  0.049888765156985884\n",
      "avg_f1_fromx2:  0.08  std_f1_fromx1:  0.049888765156985884\n",
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  15 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.08  std_f1_fromx1:  0.049888765156985884\n",
      "avg_f1_fromx2:  0.08  std_f1_fromx1:  0.049888765156985884\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for tune_time in range(len(hidden_dim_list)):\n",
    "    hidden_dim = hidden_dim_list[tune_time]\n",
    "    kfold_results_1_abla1 = []\n",
    "    kfold_results_2_abla1 = []\n",
    "    for fold, idx_list in enumerate(zip(kfold_cv_dl_list_1, kfold_cv_dl_list_2)):\n",
    "        print(\"current fold: \", fold)\n",
    "        train_idx_1 = idx_list[0][\"train_idx\"]\n",
    "        val_idx_1 = idx_list[0][\"val_idx\"]\n",
    "        train_idx_2 = idx_list[1][\"train_idx\"]\n",
    "        val_idx_2 = idx_list[1][\"val_idx\"]\n",
    "\n",
    "        train_sampler_1 = SubsetRandomSampler(train_idx_1)\n",
    "        val_sampler_1 = SubsetRandomSampler(val_idx_1)\n",
    "\n",
    "        train_sampler_2 = SubsetRandomSampler(train_idx_2)\n",
    "        val_sampler_2 = SubsetRandomSampler(val_idx_2)\n",
    "\n",
    "        train_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=train_sampler_1)\n",
    "        val_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=val_sampler_1)\n",
    "\n",
    "        train_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=train_sampler_2)\n",
    "        val_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=val_sampler_2)\n",
    "\n",
    "        trained_models_list_1 = []\n",
    "        trained_models_list_2 = []\n",
    "\n",
    "        for ump_id in range(num_ump_f_1):\n",
    "            model = ablation_model_more_rnn_layers(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                train(train_loader_1, model, optimizer)\n",
    "            trained_models_list_1.append(model)\n",
    "\n",
    "        for ump_id in range(num_ump_f_2):\n",
    "            model = ablation_model_more_rnn_layers(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                train(train_loader_2, model, optimizer)\n",
    "            trained_models_list_2.append(model)\n",
    "\n",
    "        f1_fromx1, f1_fromx2 = val(val_loader_1, val_loader_2, num_ump_f_1, num_ump_f_2, num_mp_f, seq_len, trained_models_list_1, trained_models_list_2, p)\n",
    "        kfold_results_1_abla1.append(f1_fromx1)\n",
    "        kfold_results_2_abla1.append(f1_fromx2)\n",
    "        \n",
    "    print(\"\\n ------- When hidden_dim equals to: \", hidden_dim, \"--------- \\n\")\n",
    "    print(\"avg_f1_fromx1: \", np.mean(kfold_results_1_abla1), \" std_f1_fromx1: \", np.std(kfold_results_1_abla1))\n",
    "    print(\"avg_f1_fromx2: \", np.mean(kfold_results_2_abla1), \" std_f1_fromx1: \", np.std(kfold_results_2_abla1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dca3be",
   "metadata": {},
   "source": [
    "# Ablation study 2: Only add one Dropout layer after the third linear layer (the layer before the last linear layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8fdb3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ablation_model_only_one_dp(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(ablation_model_only_one_dp, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(1, self.batch_size, hidden_dim)\n",
    "        # RNN to get final hidden states\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        # MLP to predict pre-mapped features(flatten to shape: seq_len x num_mp_f)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.MLP_drop3 = nn.Dropout(p=0.2)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop3(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fe3c9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  5 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.09333333333333334  std_f1_fromx1:  0.0679869268479038\n",
      "avg_f1_fromx2:  0.09333333333333334  std_f1_fromx1:  0.0679869268479038\n",
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  10 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.12000000000000002  std_f1_fromx1:  0.07774602526460402\n",
      "avg_f1_fromx2:  0.12000000000000002  std_f1_fromx1:  0.07774602526460402\n",
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  15 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.04  std_f1_fromx1:  0.05333333333333333\n",
      "avg_f1_fromx2:  0.04  std_f1_fromx1:  0.05333333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for tune_time in range(len(hidden_dim_list)):\n",
    "    hidden_dim = hidden_dim_list[tune_time]\n",
    "    kfold_results_1_abla2 = []\n",
    "    kfold_results_2_abla2 = []\n",
    "    for fold, idx_list in enumerate(zip(kfold_cv_dl_list_1, kfold_cv_dl_list_2)):\n",
    "        print(\"current fold: \", fold)\n",
    "        train_idx_1 = idx_list[0][\"train_idx\"]\n",
    "        val_idx_1 = idx_list[0][\"val_idx\"]\n",
    "        train_idx_2 = idx_list[1][\"train_idx\"]\n",
    "        val_idx_2 = idx_list[1][\"val_idx\"]\n",
    "\n",
    "        train_sampler_1 = SubsetRandomSampler(train_idx_1)\n",
    "        val_sampler_1 = SubsetRandomSampler(val_idx_1)\n",
    "\n",
    "        train_sampler_2 = SubsetRandomSampler(train_idx_2)\n",
    "        val_sampler_2 = SubsetRandomSampler(val_idx_2)\n",
    "\n",
    "        train_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=train_sampler_1)\n",
    "        val_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=val_sampler_1)\n",
    "\n",
    "        train_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=train_sampler_2)\n",
    "        val_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=val_sampler_2)\n",
    "\n",
    "        trained_models_list_1 = []\n",
    "        trained_models_list_2 = []\n",
    "\n",
    "        for ump_id in range(num_ump_f_1):\n",
    "            model = ablation_model_only_one_dp(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                train(train_loader_1, model, optimizer)\n",
    "            trained_models_list_1.append(model)\n",
    "\n",
    "        for ump_id in range(num_ump_f_2):\n",
    "            model = ablation_model_only_one_dp(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                train(train_loader_2, model, optimizer)\n",
    "            trained_models_list_2.append(model)\n",
    "\n",
    "        f1_fromx1, f1_fromx2 = val(val_loader_1, val_loader_2, num_ump_f_1, num_ump_f_2, num_mp_f, seq_len, trained_models_list_1, trained_models_list_2, p)\n",
    "        kfold_results_1_abla2.append(f1_fromx1)\n",
    "        kfold_results_2_abla2.append(f1_fromx2)\n",
    "    \n",
    "    print(\"\\n ------- When hidden_dim equals to: \", hidden_dim, \"--------- \\n\")\n",
    "    print(\"avg_f1_fromx1: \", np.mean(kfold_results_1_abla2), \" std_f1_fromx1: \", np.std(kfold_results_1_abla2))\n",
    "    print(\"avg_f1_fromx2: \", np.mean(kfold_results_2_abla2), \" std_f1_fromx1: \", np.std(kfold_results_2_abla2))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e6d4d",
   "metadata": {},
   "source": [
    "# Ablation study 3: Delete all the Dropout layers (only keep ReLU activation function) based on model2's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ec0b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ablation_model_no_dp_layers(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(ablation_model_no_dp_layers, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(1, self.batch_size, hidden_dim)\n",
    "        # RNN to get final hidden states\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        # MLP to predict pre-mapped features(flatten to shape: seq_len x num_mp_f)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb45ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  5 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.05333333333333333  std_f1_fromx1:  0.02666666666666667\n",
      "avg_f1_fromx2:  0.05333333333333333  std_f1_fromx1:  0.02666666666666667\n",
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  10 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.08  std_f1_fromx1:  0.049888765156985884\n",
      "avg_f1_fromx2:  0.08  std_f1_fromx1:  0.049888765156985884\n",
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  15 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.08  std_f1_fromx1:  0.049888765156985884\n",
      "avg_f1_fromx2:  0.08  std_f1_fromx1:  0.049888765156985884\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for tune_time in range(len(hidden_dim_list)):\n",
    "    hidden_dim = hidden_dim_list[tune_time]\n",
    "    kfold_results_1_abla3 = []\n",
    "    kfold_results_2_abla3 = []\n",
    "    for fold, idx_list in enumerate(zip(kfold_cv_dl_list_1, kfold_cv_dl_list_2)):\n",
    "        print(\"current fold: \", fold)\n",
    "        train_idx_1 = idx_list[0][\"train_idx\"]\n",
    "        val_idx_1 = idx_list[0][\"val_idx\"]\n",
    "        train_idx_2 = idx_list[1][\"train_idx\"]\n",
    "        val_idx_2 = idx_list[1][\"val_idx\"]\n",
    "\n",
    "        train_sampler_1 = SubsetRandomSampler(train_idx_1)\n",
    "        val_sampler_1 = SubsetRandomSampler(val_idx_1)\n",
    "\n",
    "        train_sampler_2 = SubsetRandomSampler(train_idx_2)\n",
    "        val_sampler_2 = SubsetRandomSampler(val_idx_2)\n",
    "\n",
    "        train_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=train_sampler_1)\n",
    "        val_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=val_sampler_1)\n",
    "\n",
    "        train_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=train_sampler_2)\n",
    "        val_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=val_sampler_2)\n",
    "\n",
    "        trained_models_list_1 = []\n",
    "        trained_models_list_2 = []\n",
    "\n",
    "        for ump_id in range(num_ump_f_1):\n",
    "            model = ablation_model_no_dp_layers(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                train(train_loader_1, model, optimizer)\n",
    "            trained_models_list_1.append(model)\n",
    "\n",
    "        for ump_id in range(num_ump_f_2):\n",
    "            model = ablation_model_no_dp_layers(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                train(train_loader_2, model, optimizer)\n",
    "            trained_models_list_2.append(model)\n",
    "\n",
    "        f1_fromx1, f1_fromx2 = val(val_loader_1, val_loader_2, num_ump_f_1, num_ump_f_2, num_mp_f, seq_len, trained_models_list_1, trained_models_list_2, p)\n",
    "        kfold_results_1_abla3.append(f1_fromx1)\n",
    "        kfold_results_2_abla3.append(f1_fromx2)\n",
    "        \n",
    "    print(\"\\n ------- When hidden_dim equals to: \", hidden_dim, \"--------- \\n\")\n",
    "    print(\"avg_f1_fromx1: \", np.mean(kfold_results_1_abla3), \" std_f1_fromx1: \", np.std(kfold_results_1_abla3))\n",
    "    print(\"avg_f1_fromx2: \", np.mean(kfold_results_2_abla3), \" std_f1_fromx1: \", np.std(kfold_results_2_abla3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b72d0c",
   "metadata": {},
   "source": [
    "# Ablation study 4: change the activation function in the MLP part from ReLU to LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47c154fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ablation_model_leakyReLU(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(ablation_model_leakyReLU, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(1, self.batch_size, hidden_dim)\n",
    "        # RNN to get final hidden states\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        # MLP to predict pre-mapped features(flatten to shape: seq_len x num_mp_f)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.MLP_drop1 = nn.Dropout(p=0.2)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.MLP_drop2 = nn.Dropout(p=0.2)\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.MLP_drop3 = nn.Dropout(p=0.2)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.LeakyReLU(0.1)(map_predict)\n",
    "        map_predict = self.MLP_drop1(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.LeakyReLU(0.1)(map_predict)\n",
    "        map_predict = self.MLP_drop2(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.LeakyReLU(0.1)(map_predict)\n",
    "        map_predict = self.MLP_drop3(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02b62f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  5 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.06666666666666668  std_f1_fromx1:  0.05962847939999439\n",
      "avg_f1_fromx2:  0.06666666666666668  std_f1_fromx1:  0.05962847939999439\n",
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  10 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.09333333333333334  std_f1_fromx1:  0.0679869268479038\n",
      "avg_f1_fromx2:  0.09333333333333334  std_f1_fromx1:  0.0679869268479038\n",
      "current fold:  0\n",
      "current fold:  1\n",
      "current fold:  2\n",
      "current fold:  3\n",
      "current fold:  4\n",
      "\n",
      " ------- When hidden_dim equals to:  15 --------- \n",
      "\n",
      "avg_f1_fromx1:  0.13333333333333336  std_f1_fromx1:  0.0596284793999944\n",
      "avg_f1_fromx2:  0.13333333333333336  std_f1_fromx1:  0.0596284793999944\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for tune_time in range(len(hidden_dim_list)):\n",
    "    hidden_dim = hidden_dim_list[tune_time]\n",
    "    kfold_results_1_abla4 = []\n",
    "    kfold_results_2_abla4 = []\n",
    "\n",
    "    for fold, idx_list in enumerate(zip(kfold_cv_dl_list_1, kfold_cv_dl_list_2)):\n",
    "        print(\"current fold: \", fold)\n",
    "        train_idx_1 = idx_list[0][\"train_idx\"]\n",
    "        val_idx_1 = idx_list[0][\"val_idx\"]\n",
    "        train_idx_2 = idx_list[1][\"train_idx\"]\n",
    "        val_idx_2 = idx_list[1][\"val_idx\"]\n",
    "\n",
    "        train_sampler_1 = SubsetRandomSampler(train_idx_1)\n",
    "        val_sampler_1 = SubsetRandomSampler(val_idx_1)\n",
    "\n",
    "        train_sampler_2 = SubsetRandomSampler(train_idx_2)\n",
    "        val_sampler_2 = SubsetRandomSampler(val_idx_2)\n",
    "\n",
    "        train_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=train_sampler_1)\n",
    "        val_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=val_sampler_1)\n",
    "\n",
    "        train_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=train_sampler_2)\n",
    "        val_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=val_sampler_2)\n",
    "\n",
    "        trained_models_list_1 = []\n",
    "        trained_models_list_2 = []\n",
    "\n",
    "        for ump_id in range(num_ump_f_1):\n",
    "            model = ablation_model_leakyReLU(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                train(train_loader_1, model, optimizer)\n",
    "            trained_models_list_1.append(model)\n",
    "\n",
    "        for ump_id in range(num_ump_f_2):\n",
    "            model = ablation_model_leakyReLU(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "            optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "            for epoch in range(num_epochs):\n",
    "                train(train_loader_2, model, optimizer)\n",
    "            trained_models_list_2.append(model)\n",
    "\n",
    "        f1_fromx1, f1_fromx2 = val(val_loader_1, val_loader_2, num_ump_f_1, num_ump_f_2, num_mp_f, seq_len, trained_models_list_1, trained_models_list_2, p)\n",
    "        kfold_results_1_abla4.append(f1_fromx1)\n",
    "        kfold_results_2_abla4.append(f1_fromx2)\n",
    "        \n",
    "    print(\"\\n ------- When hidden_dim equals to: \", hidden_dim, \"--------- \\n\")\n",
    "    print(\"avg_f1_fromx1: \", np.mean(kfold_results_1_abla4), \" std_f1_fromx1: \", np.std(kfold_results_1_abla4))\n",
    "    print(\"avg_f1_fromx2: \", np.mean(kfold_results_2_abla4), \" std_f1_fromx1: \", np.std(kfold_results_2_abla4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fb77b",
   "metadata": {},
   "source": [
    "# Based on above ablation studies, the seq_to_seq_model_2.0 should include below structure:\n",
    "* 1-layer RNN sturcture\n",
    "* only add one dropout layer after the Linear layer before the last Linear layer (in current case, add nn.Dropout after the third Linear Layer)\n",
    "* use ReLU() as activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eed0b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq_to_seq_model_2(nn.Module):\n",
    "    def __init__(self, ump_feature_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f):\n",
    "        super(seq_to_seq_model_2, self).__init__()\n",
    "        # each model trained on only one unmapped feature: ump_feature_id\n",
    "        # currently, ump_feature_id takes value from [0, 14]\n",
    "        self.ump_id = ump_feature_id\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.h_0 = torch.randn(1, self.batch_size, hidden_dim)\n",
    "        # RNN to get final hidden states\n",
    "        self.RNN = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        # MLP to predict pre-mapped features(flatten to shape: seq_len x num_mp_f)\n",
    "        \n",
    "        self.target_dim = num_mp_f\n",
    "        \n",
    "        self.dense1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dense2 = nn.Linear(hidden_dim, int(self.target_dim * 2))\n",
    "        self.dense3 = nn.Linear(int(self.target_dim * 2), self.target_dim)\n",
    "        self.MLP_drop3 = nn.Dropout(p=0.2)\n",
    "        self.dense4 = nn.Linear(self.target_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        true_mapped_features = input_data[0]\n",
    "        # shape: [10, 24, 5], targets\n",
    "        unmapped_features = input_data[1][:, :, self.ump_id].reshape((self.batch_size, self.seq_len, 1)) \n",
    "        # shape: [10, 24, 1], includes all unmapped features\n",
    "        output, _ = self.RNN(unmapped_features, self.h_0)\n",
    "        # output: [10, 24, hidden_dim]\n",
    "        map_predict = self.dense1(output)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense2(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        \n",
    "        map_predict = self.dense3(map_predict)\n",
    "        map_predict = nn.ReLU()(map_predict)\n",
    "        map_predict = self.MLP_drop3(map_predict)\n",
    "        \n",
    "        map_predict = self.dense4(map_predict) # shape [batch_size, seq_len * num_mp_f]\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(map_predict, true_mapped_features)\n",
    "        return {\"predicts\": map_predict, \"loss\": loss}\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b91d0e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current fold:  0\n",
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C12], R2: [C10], R3: [C3], R4: [C14], R5: [C1], R6: [C5], R7: [C7], R8: [C11], R9: [C2], R10: [C9], R11: [C4], R12: [C8], R13: [C15], R14: [C6], R15: [C13]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R12], C2: [R10], C3: [R3], C4: [R14], C5: [R1], C6: [R5], C7: [R7], C8: [R11], C9: [R2], C10: [R9], C11: [R4], C12: [R8], C13: [R15], C14: [R6], C15: [R13]}\n",
      "current fold:  1\n",
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C9], R2: [C10], R3: [C14], R4: [C5], R5: [C4], R6: [C7], R7: [C8], R8: [C1], R9: [C3], R10: [C12], R11: [C11], R12: [C15], R13: [C13], R14: [C2], R15: [C6]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R9], C2: [R10], C3: [R14], C4: [R5], C5: [R4], C6: [R7], C7: [R8], C8: [R1], C9: [R3], C10: [R12], C11: [R11], C12: [R15], C13: [R13], C14: [R2], C15: [R6]}\n",
      "current fold:  2\n",
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C5], R2: [C15], R3: [C2], R4: [C12], R5: [C14], R6: [C7], R7: [C8], R8: [C9], R9: [C11], R10: [C13], R11: [C4], R12: [C6], R13: [C1], R14: [C10], R15: [C3]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R5], C2: [R15], C3: [R2], C4: [R12], C5: [R14], C6: [R7], C7: [R8], C8: [R9], C9: [R11], C10: [R13], C11: [R4], C12: [R6], C13: [R1], C14: [R10], C15: [R3]}\n",
      "current fold:  3\n",
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C12], R2: [C14], R3: [C6], R4: [C15], R5: [C2], R6: [C11], R7: [C10], R8: [C3], R9: [C5], R10: [C9], R11: [C1], R12: [C4], R13: [C13], R14: [C7], R15: [C8]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R12], C2: [R14], C3: [R6], C4: [R15], C5: [R2], C6: [R11], C7: [R10], C8: [R3], C9: [R5], C10: [R9], C11: [R1], C12: [R4], C13: [R13], C14: [R7], C15: [R8]}\n",
      "current fold:  4\n",
      "\n",
      " ------- Matching from X1_train  --------- \n",
      "\n",
      "{R1: [C10], R2: [C1], R3: [C12], R4: [C15], R5: [C2], R6: [C6], R7: [C7], R8: [C13], R9: [C4], R10: [C9], R11: [C5], R12: [C8], R13: [C11], R14: [C14], R15: [C3]}\n",
      "\n",
      " ------- Matching from X2_train  --------- \n",
      "\n",
      "{C1: [R10], C2: [R1], C3: [R12], C4: [R15], C5: [R2], C6: [R6], C7: [R7], C8: [R13], C9: [R4], C10: [R9], C11: [R5], C12: [R8], C13: [R11], C14: [R14], C15: [R3]}\n"
     ]
    }
   ],
   "source": [
    "kfold_results_1_seq2seq_2 = []\n",
    "kfold_results_2_seq2seq_2 = []\n",
    "\n",
    "\n",
    "for fold, idx_list in enumerate(zip(kfold_cv_dl_list_1, kfold_cv_dl_list_2)):\n",
    "    print(\"current fold: \", fold)\n",
    "    train_idx_1 = idx_list[0][\"train_idx\"]\n",
    "    val_idx_1 = idx_list[0][\"val_idx\"]\n",
    "    train_idx_2 = idx_list[1][\"train_idx\"]\n",
    "    val_idx_2 = idx_list[1][\"val_idx\"]\n",
    "    \n",
    "    train_sampler_1 = SubsetRandomSampler(train_idx_1)\n",
    "    val_sampler_1 = SubsetRandomSampler(val_idx_1)\n",
    "    \n",
    "    train_sampler_2 = SubsetRandomSampler(train_idx_2)\n",
    "    val_sampler_2 = SubsetRandomSampler(val_idx_2)\n",
    "    \n",
    "    train_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=train_sampler_1)\n",
    "    val_loader_1 = DataLoader(dataset_1_all, batch_size=batch_size, sampler=val_sampler_1)\n",
    "    \n",
    "    train_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=train_sampler_2)\n",
    "    val_loader_2 = DataLoader(dataset_2_all, batch_size=batch_size, sampler=val_sampler_2)\n",
    "    \n",
    "    trained_models_list_1 = []\n",
    "    trained_models_list_2 = []\n",
    "    \n",
    "    for ump_id in range(num_ump_f_1):\n",
    "        model = seq_to_seq_model_2(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader_1, model, optimizer)\n",
    "        trained_models_list_1.append(model)\n",
    "        \n",
    "    for ump_id in range(num_ump_f_2):\n",
    "        model = seq_to_seq_model_2(ump_id, batch_size, input_dim, hidden_dim, seq_len, num_mp_f)\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        for epoch in range(num_epochs):\n",
    "            train(train_loader_2, model, optimizer)\n",
    "        trained_models_list_2.append(model)\n",
    "        \n",
    "    f1_fromx1, f1_fromx2 = val(val_loader_1, val_loader_2, num_ump_f_1, num_ump_f_2, num_mp_f, seq_len, trained_models_list_1, trained_models_list_2, p)\n",
    "    kfold_results_1_seq2seq_2.append(f1_fromx1)\n",
    "    kfold_results_2_seq2seq_2.append(f1_fromx2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4799bf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10666666666666666"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_f1_fromx1 = np.mean(kfold_results_1_seq2seq_2)\n",
    "avg_f1_fromx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a45ded7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10666666666666666"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_f1_fromx2 = np.mean(kfold_results_2_seq2seq_2)\n",
    "avg_f1_fromx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5569d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
